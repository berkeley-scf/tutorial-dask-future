[
  {
    "objectID": "python-ray.html#overview-of-ray",
    "href": "python-ray.html#overview-of-ray",
    "title": "Brief notes on parallel processing using the Ray package in Python",
    "section": "1. Overview of Ray",
    "text": "1. Overview of Ray\nThe Ray package provides a variety of tools for managing parallel computations.\nIn particular, some of the key ideas/features of Ray are:\n\nAllowing users to parallelize independent computations across multiple cores on one or more machines.\nDifferent users can run the same code on different computational resources (without touching the actual code that does the computation).\nOne nice feature relative to Dask is that Ray allows one to share data across all worker processes on a node, without multiple copies of the data, using the object store.\nRay provides tools to build distributed (across multiple cores or nodes) applications where different processes interact with each other (using the notion of ‘actors’).\n\nThese brief notes just scratch the surface of Ray and just recapitulate basic information available in the Ray documentation.",
    "crumbs": [
      "Ray in Python"
    ]
  },
  {
    "objectID": "python-ray.html#ray-on-one-machine",
    "href": "python-ray.html#ray-on-one-machine",
    "title": "Brief notes on parallel processing using the Ray package in Python",
    "section": "2. Ray on one machine",
    "text": "2. Ray on one machine\nOn one machine, we can initialize Ray from within Python.\n\nimport ray\nray.init()\n## alternatively, to specify a specific number of cores:\nray.init(num_cpus = 4)\n\nTo run a computation in parallel, we decorate the function of interest with the remote tag:\n\n@ray.remote\ndef f(x):\n    return x * x\n\nfutures = [f.remote(i) for i in range(4)]\nprint(ray.get(futures)) # [0, 1, 4, 9]",
    "crumbs": [
      "Ray in Python"
    ]
  },
  {
    "objectID": "python-ray.html#ray-on-multiple-machines-nodes",
    "href": "python-ray.html#ray-on-multiple-machines-nodes",
    "title": "Brief notes on parallel processing using the Ray package in Python",
    "section": "3. Ray on multiple machines (nodes)",
    "text": "3. Ray on multiple machines (nodes)\nHere we’ll follow the Ray instructions to start up Ray processes across multiple nodes within a Slurm job.\nMake sure to request multiple cores per node via –cpus-per-task (on Savio you’d generally set this equal to the number of cores per node).\nWe need to start the main Ray process (the Ray ‘head node’) on the head (first) node of Slurm allocation. Then we need to start one worker process for the remaining nodes (do not start a worker on the head node).\n# Getting the node names\nnodes=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\")\nnodes_array=($nodes)\n\nhead_node=${nodes_array[0]}\nhead_node_ip=$(srun --nodes=1 --ntasks=1 -w \"$head_node\" hostname --ip-address)\n\nport=6379\nip_head=$head_node_ip:$port\nexport ip_head\necho \"IP Head: $ip_head\"\n\necho \"Starting HEAD at $head_node\"\nsrun --nodes=1 --ntasks=1 -w \"$head_node\" \\\n    ray start --head --node-ip-address=\"$head_node_ip\" --port=$port \\\n    --num-cpus \"${SLURM_CPUS_PER_TASK}\" --block &\n\n# number of nodes other than the head node\nworker_num=$((SLURM_JOB_NUM_NODES - 1))\n\nfor ((i = 1; i &lt;= worker_num; i++)); do\n    node_i=${nodes_array[$i]}\n    echo \"Starting WORKER $i at $node_i\"\n    srun --nodes=1 --ntasks=1 -w \"$node_i\" \\\n        ray start --address \"$ip_head\" \\\n        --num-cpus \"${SLURM_CPUS_PER_TASK}\" --block &\n    sleep 5\ndone\nThen in Python, we need to connect to the Ray head node process:\n\nimport ray, os\nray.init(address = os.getenv('ip_head'))\n\nYou should see something like this:\n2021-03-16 14:39:48,520 INFO worker.py:654 -- Connecting to existing Ray cluster at address: 128.32.135.190:6379\n{'node_ip_address': '128.32.135.190', 'raylet_ip_address': '128.32.135.190', 'redis_address': '128.32.135.190:6379', 'object_store_address': '/tmp/ray/session_2021-03-16_14-39-26_045290_3521776/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2021-03-16_14-39-26_045290_3521776/sockets/raylet', 'webui_url': 'localhost:8265', 'session_dir': '/tmp/ray/session_2021-03-16_14-39-26_045290_3521776', 'metrics_export_port': 63983, 'node_id': '2a3f113e2093d8a8abe3e0ddc9730f8cf6b4478372afe489208b2dcf'}",
    "crumbs": [
      "Ray in Python"
    ]
  },
  {
    "objectID": "python-ray.html#using-the-ray-object-store",
    "href": "python-ray.html#using-the-ray-object-store",
    "title": "Brief notes on parallel processing using the Ray package in Python",
    "section": "4. Using the Ray object store",
    "text": "4. Using the Ray object store\nThe object store allows one to avoid making copies of data for each worker process on a node. All workers on a node can share the same data (this also avoids extra copying of data to the workers). And on top of this the object store allows one to use data in the form of numpy arrays directly using the memory allocated for the array in the object store, without any copying into a data structuer specific to the worker process.\nLet’s try this out.\n\nray.init(num_cpus = 4)   # four worker processes on the local machine\n\n@ray.remote\ndef calc(i, data):\n    import numpy as np  \n    return([np.mean(data), np.std(data)])\n\nimport numpy as np\nrng = np.random.default_rng()\n## 'y' is an 800 MB object\nn = 100000000\ny = rng.normal(0, 1, size=(n))\n\n\ny_ref = ray.put(y) # put the data in the object store\n\np = 50\n\n## One can pass the reference to the object in the object store as an argument\nfutures = [calc.remote(i, y_ref) for i in range(p)]\nray.get(futures)\n\nWe’ll watch memory use via free -h while running the test above.\nUnfortunately when I test this on a single machine, memory use seems to be equivalent to four copies of the ‘y’ object, so something seems to be wrong. And trying it on a multi-node Ray cluster doesn’t seem to clarify what is going on.\nOne can also run ray memory from the command line (not from within Python) to examine memory use in the object store. (In the case above, it simply reports the 800 MB usage.)\nOne can also create the object via a remote call and then use it.\n\n@ray.remote\ndef create(n):\n    import numpy as np\n    rng = np.random.default_rng()\n    y = rng.normal(0, 1, size=(100000000))\n    return(y)\n    \ny_ref = create.remote(n)\nfutures = [calc.remote(i, y_ref) for i in range(p)]\nray.get(futures)",
    "crumbs": [
      "Ray in Python"
    ]
  },
  {
    "objectID": "R-future.html",
    "href": "R-future.html",
    "title": "Parallel Processing using the future package in R",
    "section": "",
    "text": "What is a future? It’s basically a flag used to tag a given operation such that when and where that operation is carried out is controlled at a higher level. If there are multiple operations tagged then this allows for parallelization across those operations.\nAccording to Henrik Bengtsson (the package developer) and those who developed the concept:\n\na future is an abstraction for a value that will be available later\nthe value is the result of an evaluated expression\nthe state of a future is either unresolved or resolved\n\n\n\nThe future package allows one to write one’s computational code without hard-coding whether or how parallelization would be done. Instead one writes the code in a generic way and at the top of one’s code sets the plan for how the parallel computation should be done given the computational resources available. Simply changing the plan changes how parallelization is done for any given run of the code.\nMore concisely, the key ideas are:\n\nSeparate what to parallelize from how and where the parallelization is actually carried out.\nDifferent users can run the same code on different computational resources (without touching the actual code that does the computation).",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#overview-futures-and-the-r-future-package",
    "href": "R-future.html#overview-futures-and-the-r-future-package",
    "title": "Parallel Processing using the future package in R",
    "section": "",
    "text": "What is a future? It’s basically a flag used to tag a given operation such that when and where that operation is carried out is controlled at a higher level. If there are multiple operations tagged then this allows for parallelization across those operations.\nAccording to Henrik Bengtsson (the package developer) and those who developed the concept:\n\na future is an abstraction for a value that will be available later\nthe value is the result of an evaluated expression\nthe state of a future is either unresolved or resolved\n\n\n\nThe future package allows one to write one’s computational code without hard-coding whether or how parallelization would be done. Instead one writes the code in a generic way and at the top of one’s code sets the plan for how the parallel computation should be done given the computational resources available. Simply changing the plan changes how parallelization is done for any given run of the code.\nMore concisely, the key ideas are:\n\nSeparate what to parallelize from how and where the parallelization is actually carried out.\nDifferent users can run the same code on different computational resources (without touching the actual code that does the computation).",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#overview-of-parallel-backends",
    "href": "R-future.html#overview-of-parallel-backends",
    "title": "Parallel Processing using the future package in R",
    "section": "2. Overview of parallel backends",
    "text": "2. Overview of parallel backends\nOne uses plan() to control how parallelization is done, including what machine(s) to use and how many cores on each machine to use.\nFor example,\n\n## Spread work across multiple (all available) cores\n## based on result of `parallelly::availableCores()`:\nplan(multisession)  \n## Alternatively, explicitly can control number of workers:\nplan(multisession, workers = 4)\n\nThis table gives an overview of the different plans.\n\n\n\n\n\n\n\n\n\nType\nDescription\nMulti-node\nCopies of objects made?\n\n\n\n\nsequential\ncurrent R process (no parallelization; used for testing)\nno\nno\n\n\nmultisession\nbackground R processes\nno\nyes\n\n\nmulticore\nforked R processes (not available in Windows nor RStudio)\nno\nnot if object not modified\n\n\ncluster\nR processes on other machine(s)\nyes\nyes\n\n\n\nFor the next section (Section 3), we’ll just assume use of multisession and will provide more details on the other plans in the following section (Section 4).\nThe deprecated multiprocess plan used either multisession on Windows and multicore on MacOS/Linux. The deprecated remote plan is not needed because cluster provides equivalent functionality.",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#implementing-operations-in-parallel",
    "href": "R-future.html#implementing-operations-in-parallel",
    "title": "Parallel Processing using the future package in R",
    "section": "3. Implementing operations in parallel",
    "text": "3. Implementing operations in parallel\nThe future package has a few main patterns for how you might parallelize a computation.\n\n3.1. Parallelized lapply statements and related\nYou can parallelize lapply and related functions easily. This is a nice replacement for the confusingly similar set of such as parLapply, mclapply, and mpi.parSapply.\n\nlibrary(future.apply)\nplan(multisession)  # or some other plan\noutput &lt;- future_lapply(1:20, function(i) mean(rnorm(1e7)), future.seed = TRUE)\n# or sapply:\n# output &lt;- future_sapply(1:20, function(i) mean(rnorm(1e7)), future.seed = TRUE)\n\nAlternatively, you can use furrr:future_map to run a parallel map operation (i.e., taking an explicit functional programming perspective).\n\n\n3.2. foreach\nYou can also continue to use foreach if you like that approach.\nAs of version 1.0.0 of doFuture, you can do this, using the %dofuture% operatior.\n\nplan(multisession)  # or some other plan\n\nlibrary(doFuture, quietly = TRUE)\nregisterDoFuture()\n\nout &lt;- foreach(i = 1:5) %dofuture% {\n    cat(\"Running in process\", Sys.getpid(), \"\\n\")\n    mean(1:i)\n}\n\nRunning in process 228246 \nRunning in process 228244 \nRunning in process 228249 \nRunning in process 228247 \nRunning in process 228245 \n\nout\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 1.5\n\n[[3]]\n[1] 2\n\n[[4]]\n[1] 2.5\n\n[[5]]\n[1] 3\n\n\nPreviously, one would use this approach with the %dopar% operator:\n\nplan(multisession)  # or some other plan\n\nlibrary(dofuture, quietly = TRUE)\nregisterDoFuture()\n\nout &lt;- foreach(i = 1:5) %dopar% {\n    cat(\"Running in process\", Sys.getpid(), \"\\n\")\n    mean(1:i)\n}\nout\n\nUsing %dofuture% has various advantages listed here.\n\n\n3.3. Using futures for parallelization\nWhile future_lapply and foreach are fine, the future package introduces a new style of parallelizing code using an explicit “future”. Here we include the code for individual iterations inside future() to mark the unit of computation. The future package will then distribute the individual iterations to run in parallel, based on the plan.\n(Here the code is safe in terms of parallel randon number generation because of the seed argument - see Section 8 for more details.)\n\nplan(multisession)   # or some other plan\nn &lt;- 20\nout &lt;- list(); length(out) &lt;- n\n\nfor(i in seq_len(n)) {\n     out[[i]] &lt;- future( {\n       ## simply insert code here as you would with foreach; for example:\n       tmp &lt;- rnorm(1e7)\n       c(mean(tmp), sd(tmp))\n     }, seed = TRUE)\n}\nclass(out[[5]])\n\n[1] \"MultisessionFuture\" \"ClusterFuture\"      \"MultiprocessFuture\"\n[4] \"Future\"             \"environment\"       \n\n## Each return values (e.g., 'out[[1]]') is a wrapper, so use value() to access:\nvalue(out[[5]])\n\n[1] 0.0002898683 0.9999048541\n\n\n\n\n3.4. Using implicit futures (with listenvs)\nIn addition to using future(), one can use the special %&lt;-% operator to denote a future. The %&lt;-% operator can only operate with an environment. So we create a listenv, which is basically an environment that can be treated like a list.\nThis approach creates implicit futures, and one does not need to use value to get the result.\n(Note that, as seen in the warnings, the code here is not safe in terms of parallel randon number generation - see Section 8 for more information.)\n\nlibrary(listenv)\n\nplan(multisession, workers = 4)\nn &lt;- 20\nout &lt;- listenv()\nfor(i in seq_len(n)) {\n     out[[i]] %&lt;-% {\n       # some code here as you would with foreach; for example:\n       tmp &lt;- rnorm(1e7)\n       c(mean(tmp), sd(tmp))\n     }\n}\n\nout[[2]]\n\nWarning: UNRELIABLE VALUE: Future ('&lt;none&gt;') unexpectedly generated random\nnumbers without specifying argument 'seed'. There is a risk that those random\nnumbers are not statistically sound and the overall results might be invalid.\nTo fix this, specify 'seed=TRUE'. This ensures that proper, parallel-safe\nrandom numbers are produced via the L'Ecuyer-CMRG method. To disable this\ncheck, use 'seed=NULL', or set option 'future.rng.onMisuse' to \"ignore\".\n\n\n[1] 0.0003449889 0.9997519067\n\nout\n\nA 'listenv' vector with 20 elements (unnamed).\n\noptions(warn = -1)  ## suppress RNG warnings \nout &lt;- as.list(out)\noptions(warn = 0)\n\n\n\n3.5. Blocking and non-blocking calls\nA ‘blocking call’ prevents the user from continuing to evaluate more expressions. Often, futures are evaluated in an asynchronous way and therefore are non-blocking except for when the actual evaluated value of the expression is requested.\nHere we see that control returns to the user right away. However, asking for the value of the expression is a blocking call.\n\n## future() is non-blocking (as is %&lt;-%)\nsystem.time(\n     out &lt;- future( {\n       ## some code here as in foreach\n       tmp &lt;- rnorm(2e7)\n       c(mean(tmp), sd(tmp))\n       }, seed = TRUE)\n)\n\n   user  system elapsed \n  0.026   0.000   0.027 \n\n## Check if the calculation is done. This check is a non-blocking call.\n## That said, it's surprising it takes even 0.2 seconds. I'm not sure why.\nsystem.time(resolved(out))\n\n   user  system elapsed \n  0.000   0.000   0.011 \n\n## Get the value. This is a blocking call.\nsystem.time(value(out))\n\n   user  system elapsed \n  0.001   0.000   1.547 \n\n\n\n\nBlocking in the context of a loop over futures\nIn contrast, in a for loop, creation of additional futures is blocked if all workers are busy evaluating other futures. So in this case, evaluation of the first four futures blocks, but once the last two futures start to be evaluated, control returns to the user while those futures are evaluated in the background.\n\nplan(multisession, workers = 2)\nn &lt;- 6\nout &lt;- list(); length(out) &lt;- n\n\n## Blocked until all six futures dispatched, so blocked until first four finish.\nsystem.time(\nfor(i in seq_len(n)) {\n     out[[i]] &lt;- future( {\n       tmp &lt;- rnorm(2e7)\n       c(mean(tmp), sd(tmp))\n     }, seed = TRUE)\n})\n\n   user  system elapsed \n  0.290   0.009   3.774 \n\n## Not blocked as result already available once first four finished.\nsystem.time(value(out[[2]]))\n\n   user  system elapsed \n  0.000   0.000   0.001 \n\n## Not blocked as result already available once first four finished.\nsystem.time(value(out[[4]]))\n\n   user  system elapsed \n      0       0       0 \n\n## Blocked as results for 5th and 6th iterations are still being evaluated.\nsystem.time(value(out[[6]]))\n\n   user  system elapsed \n  0.000   0.000   1.568",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#a-tour-of-different-backends",
    "href": "R-future.html#a-tour-of-different-backends",
    "title": "Parallel Processing using the future package in R",
    "section": "4. A tour of different backends",
    "text": "4. A tour of different backends\n\n4.1. Serial (sequential) processing\nThe sequential plan allows you to run code on a single local core. This might not seem all that useful since the goal is usually to parallelize, but this helps in debugging and allows someone to run future-based code even if they only have one core available.\n\nplan(sequential)\n## future_lapply, foreach with dofuture, etc. all will still work\n\nNote that like the parallel plans, the sequential plan evaluates all code in an isolated (‘local’) environment, so your R working environment is not affected.\nActually even better for debugging is the transparent plan, which provides additional useful output.\n\n\n4.2. Multiple core processing on one machine\nWe’ve already seen that we can use the multisession plan to parallelize across the cores of one machine.\n\nplan(multisession)\n\nBy default, this will start as many workers as given in the result of parallelly::availableCores(), which should be the number of the computer or will be based on the resources available to your job, if you have a job running via a scheduler on a cluster, such as the Slurm scheduler for Linux clusters.\n\n\n4.3. Distributed processing across multiple machines via an ad hoc cluster\nIf we know the names of the machines and can access them via password-less SSH (e.g., using ssh keys), then we can simply provide the names of the machines to create a cluster and use the ‘cluster’ plan.\nHere we want to use four cores on one machine.\n\nworkers &lt;- rep('arwen.berkeley.edu', 4)\nplan(cluster, workers = workers)\n\nHere we want to use two cores on one machine and two on another.\n\nworkers &lt;- c(rep('radagast.berkeley.edu', 2), rep('gandalf.berkeley.edu', 2))\nplan(cluster, workers = workers)\n# Check we are getting workers in the right places:\nfuture_sapply(seq_along(workers), function(i) Sys.info()[['nodename']])\n\n[1] \"gandalf\"  \"gandalf\"  \"radagast\" \"radagast\"\n\n\nWe can verify that the workers are running on the various machines by checking the nodename of each of the workers:\n\ntmp &lt;- future_sapply(seq_len(nbrOfWorkers()), \n              function(i)\n                cat(\"Worker running in process\", Sys.getpid(),\n                    \"on\", Sys.info()[['nodename']], \"\\n\"))\n\n\n\n4.4. Distributed processing across multiple machines within a Slurm scheduler job\nThe future package can detect the available resources in the context of Slurm (and other schedulers). It uses parallelly::availableWorkers().\nSo you can simply call the cluster plan and get a sensible result in terms of the workers.\n\nplan(cluster)\n\nFor more manual control, if you are using Slurm and in your sbatch or srun command you use --ntasks, then the following will allow you to use as many R workers as the value of ntasks. One caveat is that one still needs to be able to access the various machines via password-less SSH.\n\nworkers &lt;- system('srun hostname', intern = TRUE)\nplan(cluster, workers = workers)\n\nIn either case, we can check that the workers are running on the various machines using the syntax in the section just above.\nNote that for this to work on the Berkeley Savio campus cluster with multiple nodes, you will probably need to load the R module via your .bashrc so that all the nodes have R and dependent modules available.\n\n\n4.5. Off-loading work to another machine\nOne can run a chunk of code on a remote machine, for example if you need a machine with more memory.\nHere’s an example where I create a plot remotely and view it locally.\n\nplan(cluster, workers = 'gandalf.berkeley.edu')\n## requires password-less SSH\n\n## future (ggplot call) is evaluated remotely\nlibrary(ggplot2)\nmydf &lt;- data.frame(y = rnorm(10), x = rnorm(10))\ng %&lt;-% { ggplot(mydf, aes(x=x, y=y)) + geom_point() }\n\n## plot locally\ng\n\n\n\n\n\n\n\n## future (ggplot call) is evaluated remotely\ng %&lt;-% R.devices::capturePlot({\n   filled.contour(volcano, color.palette = terrain.colors)\n   title(main = \"volcano data: filled contour map\")\n   })         \n\n## plot locally\ng\n\nWarning in restoreRecordedPlot(x, reloadPkgs): snapshot recorded in different R\nversion (4.4.3)",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#load-balancing-and-static-vs.-dynamic-task-allocation",
    "href": "R-future.html#load-balancing-and-static-vs.-dynamic-task-allocation",
    "title": "Parallel Processing using the future package in R",
    "section": "5. Load-balancing and static vs. dynamic task allocation",
    "text": "5. Load-balancing and static vs. dynamic task allocation\n\nfuture_lapply uses static (non-load-balanced) allocation:\n\nGroups iterations into tasks and creates only as many tasks as there are workers.\nThis is good for reducing overhead but can potentially result in bad load-balancing if the tasks assigned to one worker take a very different time to complete from those assigned to a different worker.\nThe future.scheduling argument gives the user control over using static vs. dynamic allocation.\n\nExplicit or implicit futures use dynamic (load-balanced) allocation:\n\nInvolves dispatching one task per iteration, with the resulting overhead (i.e., latency).\nGood for load-balancing when tasks take very different times to complete.",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#avoiding-copies-when-doing-multi-process-parallelization-on-a-single-node",
    "href": "R-future.html#avoiding-copies-when-doing-multi-process-parallelization-on-a-single-node",
    "title": "Parallel Processing using the future package in R",
    "section": "6. Avoiding copies when doing multi-process parallelization on a single node",
    "text": "6. Avoiding copies when doing multi-process parallelization on a single node\nThe future package automatically identifies the objects needed by your future-based code and makes copies of those objects for use in the worker processes.\nIf you’re working with large objects, making a copy of the objects for each of the worker processes can be a significant time cost and can greatly increase your memory use.\nOn non-Windows machines, the multicore plan (not available on Windows or in RStudio) forks the main R process. This creates R worker processes with the same state as the original R process.\n\nInterestingly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process.\nSo the additional processes do not use additional memory for those objects (despite what is shown in top as memory used by each process).\nAnd there is no time involved in making copies.\nHowever, if you modify objects in the worker processes then copies are made.\nYou can use these global variables in functions you call in parallel or pass the variables into functions as function arguments.\n\nSo, the take-home message is that using multicore on non-Windows machines can have a big advantage when working with large data objects.\nUnfortunately, the top program on Linux or MacOS will make it look like additional memory is being used. Instead, on a Linux machine, you can use the command free -h to see the usable remaining memory. Look under the ‘Available’ column of the ‘Mem’ row. For example, below we see 6.5 GB available and 8.2 GB used. (Note that the ‘free’ column omits memory that is actually available but is temporarily in use for caching.)\npaciorek@smeagol:~/staff/workshops/r-future&gt; free -h\n              total        used        free      shared  buff/cache   available\nMem:            15G        8.2G        4.3G        453M        3.1G        6.5G\nSwap:          7.6G        2.9G        4.7G\nHere’s some demo code to show that forking uses less memory.\nFirst run free via watch in one terminal so you can monitor free -h while running the R code:\n\nwatch -n 0.1 free -h\n\nNow you can try running this code either with (using multicore) or without (using multisession) forking:\n\n## allow total size of global variables to be large enough...\noptions(future.globals.maxSize = 1e9)\n\n## Try with multicore:\nx &lt;- rnorm(5e7)\nplan(multicore, workers = 3)  # forks; no copying!\nsystem.time(tmp &lt;- future_sapply(1:100, function(i) mean(x)))\n\n## Try with multisession:\nx &lt;- rnorm(5e7)\nplan(multisession, workers = 3) # new processes - copying!\nsystem.time(tmp &lt;- future_sapply(1:100, function(i) mean(x)))",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#nested-futuresfor-loops",
    "href": "R-future.html#nested-futuresfor-loops",
    "title": "Parallel Processing using the future package in R",
    "section": "7. Nested futures/for loops",
    "text": "7. Nested futures/for loops\nYou can set up nested parallelization and use various plans to parallelize at each level.\nFor example, suppose you are running a simulation study with four scenarios and each scenario involving n simulations. In that case you have two loops over which you could choose to parallelize.\nHere’s some syntax to set up parallelization over the scenarios (outer loop) only. Note that when the plan involves multiple levels we need to use tweak if we want to modify the defaults for a type of future.\n\nplan(list(tweak(multisession, workers = 4), sequential))\n\nparams &lt;- cbind(c(0,0,1,1), c(1,2,1,2))\np &lt;- nrow(params)  # 4 in this case\nn &lt;- 10\nout &lt;- listenv()\nfor(k in seq_len(p)) {                         # outer loop: parameter sweep\n     out[[k]] &lt;- future( {    \n        out_single_param &lt;- listenv()\n        for(i in seq_len(n)) {                 # inner loop: replications\n          out_single_param[[i]] %&lt;-% {\n            tmp &lt;- rnorm(2e7, params[k, 1], params[k, 2])\n            c(mean(tmp), sd(tmp))\n          }\n        }\n        matrix(unlist(out_single_param), ncol = 2, byrow = TRUE)\n     }, seed = TRUE)\n}\n## non-blocking - note that control returns to the user since we have\n## four outer iterations and four workers\nout\n\nA 'listenv' vector with 4 elements (unnamed).\n\n## asking for an actual value is a blocking call\nout[[1]]\n\nMultisessionFuture:\nLabel: '&lt;none&gt;'\nExpression:\n{\n    out_single_param &lt;- listenv()\n    for (i in seq_len(n)) {\n        out_single_param[[i]] %&lt;-% {\n            tmp &lt;- rnorm(2e+07, params[k, 1], params[k, 2])\n            c(mean(tmp), sd(tmp))\n        }\n    }\n    matrix(unlist(out_single_param), ncol = 2, byrow = TRUE)\n}\nLazy evaluation: FALSE\nAsynchronous evaluation: TRUE\nLocal evaluation: TRUE\nEnvironment: R_GlobalEnv\nCapture standard output: TRUE\nCapture condition classes: 'condition' (excluding 'nothing')\nGlobals: 3 objects totaling 392 bytes (numeric 'n' of 56 bytes, matrix 'params' of 280 bytes, integer 'k' of 56 bytes)\nPackages: 3 packages ('listenv', 'stats', 'future')\nL'Ecuyer-CMRG RNG seed: c(10407, 685259277, -1301327100, -794103175, 644605022, -1466457024, 113265414)\nResolved: FALSE\nValue: &lt;not collected&gt;\nConditions captured: &lt;none&gt;\nEarly signaling: FALSE\nOwner process: 760598a1-7676-01da-5744-4cfc7c88fba3\nClass: 'MultisessionFuture', 'ClusterFuture', 'MultiprocessFuture', 'Future', 'environment'\n\n\nNote that these are “asynchronous” futures that are evaluated in the background while control returns to the user.\n\n7.1. Nested futures/for loops - some example plans\nLet’s see a few different plans one could use for the nested loops.\nTo use eight cores on the current machine, two cores for each of the four outer iteration:\n\n## One option:\nplan(list(tweak(multisession, workers = 4), tweak(multisession, workers = 2)))\n## Another option\nnodes &lt;- rep('localhost', 4)\nplan(list(tweak(cluster, workers = nodes), tweak(multisession, workers = 2)))\n\nTo run each parameter across as many workers as are available on each of multiple machines:\n\nnodes &lt;- c('dorothy.berkeley.edu', 'radagast.berkeley.edu', 'gandalf.berkeley.edu')\nplan(list(tweak(cluster, workers = nodes), multisession))\n\nNote that you can’t use a multicore future at multiple levels as future prevents nested multicore parallelization.\nIf there are many inner iterations and few outer iterations, we might simply do the outer iterations sequentially:\n\nplan(list(sequential, multisession))\n\nSee the future vignette on topologies for more configuration examples.\nNested parallelization via future is flexible: once you’ve set your code to allow parallelization at multiple levels, you can change the plan without ever touching the core code again.\n\n\n7.2. Hybrid parallelization: multiple processes plus threaded linear algebra\nIf you have access to threaded BLAS (e.g., MKL or openBLAS), as long as you set OMP_NUM_THREADS greater than 1, then any linear algebra should be parallelized within each of the iterations in a loop or apply statement.",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#reliable-random-number-generation-rng",
    "href": "R-future.html#reliable-random-number-generation-rng",
    "title": "Parallel Processing using the future package in R",
    "section": "8. Reliable random number generation (RNG)",
    "text": "8. Reliable random number generation (RNG)\nIn the code above, I was sometimes cavalier about the seeds for the random number generation in the different parallel computations.\nThe general problem is that we want the random numbers generated on each worker to not overlap with the random numbers generated on other workers. But random number generation involves numbers from a periodic sequence. Simply setting different seeds on different workers does not guarantee non-overlapping random numbers (though in most cases they probably would not overlap).\nThe future package integrates well with the L’Ecuyer parallel RNG approach, which guarantees non-overlapping random numbers. There is a good discussion about seeds for future_lapply and future_sapply in the help for those functions.\n\n8.1. The seed for future_lapply\nHere we can set a single seed. Behind the scenes the L’Ecuyer-CMRG RNG is used so that the random numbers generated for each iteration are independent. Note there is some overhead here when the number of iterations is large.\n\nlibrary(future.apply)\nn &lt;- 4\nset.seed(1)\nfuture_sapply(1:n, function(i) rnorm(1), future.seed = TRUE)\n\n[1]  1.3775667 -1.7371292 -0.1362109  1.9301162\n\nset.seed(1)\nfuture_sapply(1:n, function(i) rnorm(1), future.seed = TRUE)\n\n[1]  1.3775667 -1.7371292 -0.1362109  1.9301162\n\n\nBasically future_lapply pregenerates a seed for each iteration using parallel:::nextRNGStream, which uses the L’Ecuyer algorithm. See more details on seeds with future here.\nI could also have set future.seed = 1 instead of setting the seed using set.seed to make the generated results reproducible.\n\n\n8.2. The seed when using futures directly\nYou can (and should when using RNG) set the seed in future(). As of version 1.24.0 of the future package, the following works fine to safely set different seeds for the different workers.\n\nplan(multisession,workers=4)   # or some other plan\n\nset.seed(1)\nn &lt;- 5\nout &lt;- list(); length(out) &lt;- n\nfor(i in seq_len(n)) {\n     out[[i]] &lt;- future( {\n       ## some code here as in foreach\n       tmp &lt;- rnorm(1e7)\n       c(mean(tmp), sd(tmp))\n     }, seed = TRUE)\n}\nsapply(out, value)\n\n              [,1]         [,2]          [,3]         [,4]          [,5]\n[1,] -0.0003571476 0.0005987421 -0.0003570343 0.0005989737 -0.0003572973\n[2,]  0.9998949844 1.0002698980  0.9998949304 1.0002696339  0.9998947676\n\n\nAnd here with implicit futures:\n\nlibrary(listenv)\nset.seed(1)\n\nn &lt;- 5\nout &lt;- listenv()\nfor(i in seq_len(n)) {\n     out[[i]] %&lt;-% {\n       ## some code here as in foreach\n       tmp &lt;- rnorm(1e7)\n       c(mean(tmp), sd(tmp))\n     } %seed% TRUE\n}\ndo.call(cbind, as.list(out))\n\n              [,1]         [,2]          [,3]         [,4]          [,5]\n[1,] -0.0003571476 0.0005987421 -0.0003570343 0.0005989737 -0.0003572973\n[2,]  0.9998949844 1.0002698980  0.9998949304 1.0002696339  0.9998947676\n\n\n\n\n8.3. The seed with foreach\nWhen using %dofuture%, you can simply include .options.future = list(seed = TRUE) to ensure parallel RNG is done safely. If you forget and have RNG in your parallelized code, %dofuture% will warn you.\nBefore version 1.0.0 of doFuture, one would need to use the %doRNG% operator with foreach to ensure correct RNG with foreach.",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#submitting-slurm-jobs-from-future-using-batchtools",
    "href": "R-future.html#submitting-slurm-jobs-from-future-using-batchtools",
    "title": "Parallel Processing using the future package in R",
    "section": "9. Submitting Slurm jobs from future using batchtools",
    "text": "9. Submitting Slurm jobs from future using batchtools\nWe can use the future.batchtools package to submit jobs to a cluster scheduler from within R.\n\n9.1. One Slurm job per worker\nOne downside is that this submits one job per worker. On clusters (such as Savio) that schedule an entire node at once, that won’t work.\nOn the SCF it is fine (so long as you don’t have, say, tens of thousands of jobs). Here’s an example. Note that the resources argument tells what the Slurm arguments should be for each worker.\n\nlibrary(future.apply)\nlibrary(future.batchtools)\n## Starts five workers as separate jobs.\nplan(batchtools_slurm, workers = 5,\n                       resources = list(nodes = \"1\", ntasks = \"1\",\n                       cpus_per_task = \"1\", walltime = \"00:05:00\"),\n                       template = \"batchtools.slurm.tmpl\")\n\noutput &lt;- future_sapply(1:100, function(i) mean(rnorm(1e7)), future.seed = 1)\n\n\n\n9.2. Submitting Slurm jobs that are allocated per node\nYou can use nested futures to deal with the one job per worker issue. Here the outer future is just a wrapper to allow the overall code to be run within a single Slurm job.\n\nlibrary(future.apply)\nlibrary(future.batchtools)\nnumWorkers &lt;- 5\n## five workers\nplan(list(tweak(batchtools_slurm, workers = 1,\n                       resources = list(\n                                 nodes = \"1\",\n                                 ntasks = as.character(numWorkers),\n                                 cpus_per_task = \"1\",\n                                 partition = \"high\",\n                                 walltime = \"00:05:00\"),\n                       template = \"batchtools.slurm.tmpl\"),\n          tweak(multisession, workers = numWorkers)))\n\n## Now make sure to call `future_sapply` within an outer call to `future()`:\nmyfuture &lt;- future({ future_sapply(1:100, function(i) mean(rnorm(1e7)),\n                                   future.seed = 1) })\nout &lt;- value(myfuture)\n\nWhile this is feasible, I prefer to set up my cluster jobs outside of R and have the R code not have to know anything about how the scheduler works or what scheduler is available on a given cluster.",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "R-future.html#futurizing-your-code",
    "href": "R-future.html#futurizing-your-code",
    "title": "Parallel Processing using the future package in R",
    "section": "10. Futurizing your code",
    "text": "10. Futurizing your code\nOf course even with the future package one would generally need to write the code in anticipation of what might be parallelized.\nHowever, in the case of lapply and sapply, you could even do this to “futurize” someone else’s code:\n\nlapply &lt;- future_lapply\nsapply &lt;- future_sapply\n\nand then just set a plan and run, since the arguments to future_lapply are the same as lapply.\nNote it’s not possible to do this with parLapply as it requires passing a cluster object as an argument, but something like this would be possible with mclapply.",
    "crumbs": [
      "future in R"
    ]
  },
  {
    "objectID": "python-dask.html",
    "href": "python-dask.html",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "",
    "text": "The Dask package provides a variety of tools for managing parallel computations.\nIn particular, some of the key ideas/features of Dask are:\n\nSeparate what to parallelize from how and where the parallelization is actually carried out.\nDifferent users can run the same code on different computational resources (without touching the actual code that does the computation).\nDask provides distributed data structures that can be treated as a single data structures when runnig operations on them (like Spark and pbdR).\n\nThe idea of a ‘future’ or ‘delayed’ operation is to tag operations such that they run lazily. Multiple operations can then be pipelined together and Dask can figure out how best to compute them in parallel on the computational resources available to a given user (which may be different than the resources available to a different user).\nLet’s import dask to get started.\n\nimport dask",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#overview-of-dask",
    "href": "python-dask.html#overview-of-dask",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "",
    "text": "The Dask package provides a variety of tools for managing parallel computations.\nIn particular, some of the key ideas/features of Dask are:\n\nSeparate what to parallelize from how and where the parallelization is actually carried out.\nDifferent users can run the same code on different computational resources (without touching the actual code that does the computation).\nDask provides distributed data structures that can be treated as a single data structures when runnig operations on them (like Spark and pbdR).\n\nThe idea of a ‘future’ or ‘delayed’ operation is to tag operations such that they run lazily. Multiple operations can then be pipelined together and Dask can figure out how best to compute them in parallel on the computational resources available to a given user (which may be different than the resources available to a different user).\nLet’s import dask to get started.\n\nimport dask",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#overview-of-parallel-schedulers",
    "href": "python-dask.html#overview-of-parallel-schedulers",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "2. Overview of parallel schedulers",
    "text": "2. Overview of parallel schedulers\nOne specifies a “scheduler” to control how parallelization is done, including what machine(s) to use and how many cores on each machine to use.\nFor example,\n\nimport dask.multiprocessing\n# spread work across multiple cores, one worker per core\ndask.config.set(scheduler='processes', num_workers = 4)  \n\nThis table gives an overview of the different scheduler.\n\n\n\nType\nDescription\nMulti-node\nCopies of objects made?\n\n\n\n\nsynchronous\nnot in parallel\nno\nno\n\n\nthreaded\nthreads within current Python session\nno\nno\n\n\nprocesses\nbackground Python sessions\nno\nyes\n\n\ndistributed\nPython sessions across multiple nodes\nyes or no\nyes\n\n\n\nNote that because of Python’s Global Interpreter Lock (GIL), many computations done in pure Python code won’t be parallelized using the ‘threaded’ scheduler; however computations on numeric data in numpy arrays, Pandas dataframes and other C/C++/Cython-based code will parallelize.\nFor the next section (Section 3), we’ll just assume use of the ‘processes’ schduler and will provide more details on the other schedulers in the following section (Section 4).",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#implementing-operations-in-parallel-by-hand",
    "href": "python-dask.html#implementing-operations-in-parallel-by-hand",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "3. Implementing operations in parallel “by hand”",
    "text": "3. Implementing operations in parallel “by hand”\nDask has a large variety of patterns for how you might parallelize a computation.\nWe’ll simply parallelize computation of the mean of a large number of random numbers across multiple replicates as can be seen in calc_mean.py.\n\nfrom calc_mean import *\n\n(Note the code in calc_mean.py is not safe in terms of parallel random number generation - see Section 8 later in this document.)\n\n3.1. Using a ‘future’ via ‘delayed’\nThe basic pattern for setting up a parallel loop is:\n\n3.1.1 For loop\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  \n\nfutures = []\nn = 10000000\np = 10\nfor i in range(p):\n    futures.append(dask.delayed(calc_mean)(i, n))  # add lazy task\n\nfutures\nresults = dask.compute(futures)  # compute all in parallel\n\n\n\n3.1.2 List comprehension\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  \n\nn = 10000000\np = 10\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nfutures\nresults = dask.compute(futures)\n\nYou could set the scheduler in the compute call:\n\nresults = dask.compute(futures, scheduler = 'processes')\n\nHowever, it is best practice to separate what is parallelized from where the parallelization is done, specifying the scheduler at the start of your code.\n\n\n\n3.2. Parallel maps\nWe can do parallel map operations (i.e., a map in the map-reduce or functional programming sense, akin to lapply in R).\nFor this we need to use the distributed scheduler, which we’ll discuss more later. Note that the distributed scheduler can work on one node or on multiple nodes.\n\n# Ignore this setup for now; we'll see it again later\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = 4)\nc = Client(cluster)\n\n# Set up and execute the parallel map\n# We need the input for each function call to be a single object,\n# so use the vectorized version of calc_mean\ninputs = [(i, n) for i in range(p)]\n# execute the function across the array of input values\nfuture = c.map(calc_mean_vargs, inputs)\nresults = c.gather(future)\nresults\n\nThe map operation appears to cache results. If you rerun the above with the same inputs, you get the same result back essentially instantaneously (even if one removes the setting of the seed from calc_mean_vargs). HOWEVER, that means that if there is randomness in the results of your function for a given input, Dask will just continue to return the original output.\n\n\n3.3. Delayed evaluation and task graphs\nYou can use delayed in more complicated situations than the simple iterations shown above.\n\ndef inc(x):\n    return x + 1\n\ndef add(x, y):\n    return x + y\n\nx = dask.delayed(inc)(1)\ny = dask.delayed(inc)(2)\nz = dask.delayed(add)(x, y)\nz.compute()\n\nz.visualize(filename = 'task_graph.svg')\n\nvisualize() uses the graphviz package to illustrate the task graph (similar to a directed acyclic graph in a statistical model and to how Tensorflow organizes its computations).\nOne can also tell Dask to always delay evaluation of a given function:\n\n@dask.delayed\ndef inc(x):\n    return x + 1\n\n@dask.delayed\ndef add(x, y):\n    return x + y\n\nx = inc(1)\ny = inc(2)\nz = add(x, y)\nz.compute()\n\n\n\n3.4. The Futures interface\nYou can also control evaluation of tasks using the Futures interface for managing tasks. Unlike use of delayed, the evaluation occurs immediately instead of via lazy evaluation.",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#dask-distributed-datastructures-and-automatic-parallel-operations-on-them",
    "href": "python-dask.html#dask-distributed-datastructures-and-automatic-parallel-operations-on-them",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "4. Dask distributed datastructures and “automatic” parallel operations on them",
    "text": "4. Dask distributed datastructures and “automatic” parallel operations on them\nDask provides the ability to work on data structures that are split (sharded/chunked) across workers. There are two big advantages of this:\n\nYou can do calculations in parallel because each worker will work on a piece of the data.\nWhen the data is split across machines, you can use the memory of multiple machines to handle much larger datasets than would be possible in memory on one machine. That said, Dask processes the data in chunks, so one often doesn’t need a lot of memory, even just on one machine.\n\nBecause computations are done in external compiled code (e.g., via numpy) it’s effective to use the threaded scheduler when operating on one node to avoid having to copy and move the data.\n\n4.1. Dataframes (pandas)\nDask dataframes are Pandas-like dataframes where each dataframe is split into groups of rows, stored as smaller Pandas dataframes.\nOne can do a lot of the kinds of computations that you would do on a Pandas dataframe on a Dask dataframe, but many operations are not possible, as discussed in the Dask dataframe API.\nBy default dataframes are handled by the threads scheduler.\nHere’s an example of reading from a dataset of flight delays (about 11 GB data). You can get the flight delay data.\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.dataframe as ddf\nair = ddf.read_csv('/scratch/users/paciorek/243/AirlineData/csvs/*.csv.bz2',\n      compression = 'bz2',\n      encoding = 'latin1',   # (unexpected) latin1 value(s) 2001 file TailNum field\n      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n      'TailNum': 'object', 'CancellationCode': 'object'})\n# specify dtypes so Pandas doesn't complain about column type heterogeneity\n      \nair.DepDelay.max().compute()   # this takes a while (6 minutes with 8 cores on an SCF server)\nsub = air[(air.UniqueCarrier == 'UA') & (air.Origin == 'SFO')]\nbyDest = sub.groupby('Dest').DepDelay.mean()\nbyDest.compute()               # this takes a while too\n\nYou should see this:\n\nDest\nACV    26.200000\nBFL     1.000000\nBOI    12.855069\nBOS     9.316795\nCLE     4.000000\n...\n\n\n\n4.2. Bags\nBags are like lists but there is no particular ordering, so it doesn’t make sense to ask for the i’th element.\nYou can think of operations on Dask bags as being like parallel map operations on lists in Python or R.\nBy default bags are handled via the multiprocessing scheduler.\nLet’s see some basic operations on a large dataset of Wikipedia log files. You can get a subset of the Wikipedia data.\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  # multiprocessing is the default\nimport dask.bag as db\nwiki = db.read_text('/scratch/users/paciorek/wikistats/dated_2017/part-0000*gz')\nimport time\nt0 = time.time()\nwiki.count().compute()\ntime.time() - t0   # 136 sec.\n\nimport re\ndef find(line, regex = \"Obama\", language = \"en\"):\n    vals = line.split(' ')\n    if len(vals) &lt; 6:\n        return(False)\n    tmp = re.search(regex, vals[3])\n    if tmp is None or (language != None and vals[2] != language):\n        return(False)\n    else:\n        return(True)\n    \n\nobama = wiki.filter(find).compute()\nobama[0:5]\n\nThat should look like this:\n\n['20081113 100000 en Image:Flickr_Obama_Springfield_01.jpg 25 306083', '20081004 130000 en Special:FilePath/Opinion_polling_for_the_United_States_presidential_election,_2008,_Barack_Obama_Fred_Thompson.png 1 1224', '20081004 130000 en Special:FilePath/Opinion_polling_for_the_United_States_presidential_election,_2008,_Barack_Obama_Rudy_Giuliani.png 1 1212', '20081217 160000 en File:Michelle,_Malia_and_Sasha_Obama_at_DNC.jpg 7 97330', '20081217 160000 en File:Michelle,_Oprah_Winfrey_and_Barack_Obama.jpg 6 120260']\n\nNote that it is quite inefficient to do the find() (and therefore necessarily reading the data in) and then compute on top of that intermediate result in two separate calls to compute(). More in Section 6.\n\n\n\n\n\n\nMemory use danger\n\n\n\nBe careful about running compute such that it produces a large object in the main Python session.\nE.g., above you would not want to do data = wiki.compute() as that would pull the entire dataset into your main Python session as a single (very large) list.\n\n\n\n\n4.3. Arrays (numpy)\nDask arrays are numpy-like arrays where each array is split up by both rows and columns into smaller numpy arrays.\nOne can do a lot of the kinds of computations that you would do on a numpy array on a Dask array, but many operations are not possible, as discussed in the Dask array API.\nBy default arrays are handled via the threads scheduler.\n\n4.3.1 Arrays on a single node/machine\nLet’s first see operations on a single node, using a single 13 GB 2-d array. Note that Dask uses lazy evaluation, so creation of the array doesn’t happen until an operation requiring output is done.\nHere we specify that the chunks (the sub-arrays) are 10000 by 10000.\n\nimport dask\ndask.config.set(scheduler = 'threads', num_workers = 4) \nimport dask.array as da\nx = da.random.normal(0, 1, size=(40000,40000), chunks=(10000, 10000))\n# square 10k x 10k chunks\nmycalc = da.mean(x, axis = 1)  # by row\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0  # 41 sec.\n\nFor a row-based operation, we would presumably only want to chunk things up by row, but this doesn’t seem to actually make a difference, presumably because the mean calculation can be done in pieces and only a small number of summary statistics moved between workers.\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\n# Set so that each chunk has 2500 rows and all columns\n# x = da.from_array(x, chunks=(2500, 40000))  # how to adjust chunk size of existing array\nx = da.random.normal(0, 1, size=(40000,40000), chunks=(2500, 40000))  \nmycalc = da.mean(x, axis = 1)  # row means\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 42 sec.\n\nOf course, given the lazy evaluation, this timing comparison is not just timing the actual row mean calculations.\nBut this doesn’t really clarify the story…\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\nimport numpy as np\nimport time\nt0 = time.time()\nrng = np.random.default_rng()\nx = rng.normal(0, 1, size=(40000,40000))\ntime.time() - t0   # 110 sec.\n# for some reason the from_array and da.mean calculations are not done lazily here\nt0 = time.time()\ndx = da.from_array(x, chunks=(2500, 40000))\ntime.time() - t0   # 27 sec.\nt0 = time.time()\nmycalc = da.mean(x, axis = 1)  # what is this doing given .compute() also takes time?\ntime.time() - t0   # 28 sec.\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 21 sec.\n\nDask will avoid storing all the chunks in memory. (It appears to just generate them on the fly.) Here we have an 80 GB array but we never use more than a few GB of memory (based on top or free -h).\n\nimport dask\ndask.config.set(scheduler='threads', num_workers = 4)  \nimport dask.array as da\nx = da.random.normal(0, 1, size=(100000,100000), chunks=(10000, 10000))\nmycalc = da.mean(x, axis = 1)  # row means\nimport time\nt0 = time.time()\nrs = mycalc.compute()\ntime.time() - t0   # 205 sec.\nrs[0:5]\n\n\n\n4.3.2 Arrays split across multiple nodes/machines\nThis should be straightforward based on using Dask distributed. However, one would want to be careful about creating arrays by distributing the data from a single Python process as that would involve copying between machines.",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#using-different-schedulers",
    "href": "python-dask.html#using-different-schedulers",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "5. Using different schedulers",
    "text": "5. Using different schedulers\n\n5.1. Using threads (no copying)\n\ndask.config.set(scheduler='threads', num_workers = 4)  \nn = 100000000\np = 4\n\n## Using current numpy random number generator\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\n\nt0 = time.time()\nresults = dask.compute(futures)\ntime.time() - t0    # 3.4 sec.\n\ndef calc_mean_old(i, n):\n    import numpy as np\n    data = np.random.normal(size = n)\n    return([np.mean(data), np.std(data)])\n\n## Using deprecated numpy random number generator\nfutures = [dask.delayed(calc_mean_old)(i, n) for i in range(p)]\n\nt0 = time.time()\nresults = dask.compute(futures)\ntime.time() - t0    # 21 sec.\n\nThe computation here effectively parallelizes. However, if instead of using the default_rng Generator constructor, one uses the old numpy syntax of np.random.normal(size = n), one would see it takes four times as long, so is not parallelizing.\nThe problem is presumably occurring because of Python’s Global Interpreter Lock (GIL): any computations done in pure Python code can not be parallelized using the ‘threaded’ scheduler. However, computations on numeric data in numpy arrays, pandas dataframes and other C/C++/Cython-based code would parallelize.\nExactly why one form of numpy code encounters the GIL and the other is not clear to me.\n\n\n5.2. Multi-process parallelization via Dask Multiprocessing\nWe can effectively parallelize regardless of the GIL by using multiple Python processes.\n\nimport dask.multiprocessing\ndask.config.set(scheduler='processes', num_workers = 4)  \nn = 100000000\np = 4\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\n\n\nt0 = time.time()\nresults = dask.compute(futures)\ntime.time() - t0  # 4.0 sec.\n\n\n\n\n\n\n\nInteractive vs. background use\n\n\n\nThe above code will work when run in an interactive Python session. However, if you want to run it within a Python script (i.e., in a background/batch job), you’ll need to configure the scheduler and run the code within an if __name__ == '__main__' block:\n\nimport dask.multiprocessing\n\nif __name__ == '__main__':\n    dask.config.set(scheduler='processes', num_workers = 4)\n    n = 100000000\n    p = 4\n    futures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\n    \n    t0 = time.time()\n    results = dask.compute(futures)\n    time.time() - t0  # 4.0 sec.\n\n\n\n\n\n5.3. Multi-process parallelization via Dask Distributed (local)\nAccording to the Dask documentation, using Distributed on a local machine has advantages over multiprocessing, including the diagnostic dashboard (see Section 7) and better handling of when copies need to be made. As we saw previously, using Distributed also allows us to use the handy map() operation.\n\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = 4)\nc = Client(cluster)\n\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nt0 = time.time()\nresults = dask.compute(futures)\ntime.time() - t0   # 3.4 sec.\n\n\n\n\n\n\n\nInteractive vs. background use\n\n\n\nThe above code will work when run in an interactive Python session. However, if you want to run it within a Python script (i.e., in a background/batch job), you’ll need to configure the scheduler and run the code within an if __name__ == '__main__' block:\n\nfrom dask.distributed import Client, LocalCluster\n\nif __name__ == '__main__':\n    from calc_mean import *\n\n    cluster = LocalCluster(n_workers = 4)\n    c = Client(cluster)\n\n    futures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\n    t0 = time.time()\n    results = dask.compute(futures)\n    time.time() - t0   # 7 sec.\n\n\n\n\n\n5.4. Distributed processing across multiple machines via an ad hoc cluster\nWe need to set up a scheduler on one machine (possibly the machine we are on) and workers on whatever machines we want to do the computation on.\nOne option is to use the dask-ssh command to start up the scheduler and workers. (Note that for this to work we need to have password-less SSH working to connect to the various machines.)\nexport SCHED=$(hostname)\ndask-ssh --scheduler ${SCHED} radagast.berkeley.edu radagast.berkeley.edu arwen.berkeley.edu arwen.berkeley.edu &\n## or:\n## echo -e \"radagast.berkeley.edu radagast.berkeley.edu arwen.berkeley.edu arwen.berkeley.edu\" &gt; .hosts\n## dask-ssh --scheduler ${SCHED} --hostfile .hosts\nThen in Python, connect to the cluster via the scheduler.\n\nfrom dask.distributed import Client\nimport os\nc = Client(address = os.getenv('SCHED') + ':8786')\nc.upload_file('calc_mean.py')  # make module accessible to workers\nn = 100000000\np = 4\n\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nresults = dask.compute(futures)\n# The following seems to work to kill the worker processes, but errors are reported...\nc.shutdown()\n\nAlternatively, you can start the workers from within Python:\n\nfrom dask.distributed import Client, SSHCluster\n# first host is the scheduler\ncluster = SSHCluster(\n    [\"gandalf.berkeley.edu\", \"radagast.berkeley.edu\", \"radagast.berkeley.edu\", \"arwen.berkeley.edu\", \"arwen.berkeley.edu\"]\n)\nc = Client(cluster)\n\n# now do your computations....\n\nc.shutdown()\n\n\n\n5.5. Distributed processing using multiple machines within a Slurm scheduler job\nTo run within a Slurm job we can use dask-ssh or a combination of dask-scheduler and dask-worker..\nProvided that we have used –ntasks or –nodes and –ntasks-per-node to set the number of CPUs desired (and not –cpus-per-task), we can use srun to enumerate where the workers should run.\nFirst we’ll start the scheduler and the workers.\nexport SCHED=$(hostname):8786\ndask-scheduler&\nsleep 50   \n# On the UC Berkeley Savio cluster, I've gotten issues with the local directory being in my home directory,\n# so use /tmp\nsrun dask-worker tcp://${SCHED} --local-directory /tmp &   # might need machinename.berkeley.edu:8786\nsleep 100   # might need even more time to make sure workers start up fully\nThen in Python, connect to the cluster via the scheduler.\n\nimport os, time, dask\nfrom dask.distributed import Client\nc = Client(address = os.getenv('SCHED'))\nc.upload_file('calc_mean.py')  # make module accessible to workers\nn = 100000000\np = 24\n\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nt0 = time.time()\nresults = dask.compute(futures)\ntime.time() - t0\n\nLet’s process the 500 GB of Wikipedia log data. (Note that when I tried this on our local department cluster, I had some errors that might be related to the SCF not being set up for fast parallel I/O.) You’ll need to change the path to the data if using this code yourself.\n\nimport os\nfrom dask.distributed import Client\nc = Client(address = os.getenv('SCHED'))\nimport dask.bag as db\nwiki = db.read_text('wikistats_full/dated/part*')\nimport time\nt0 = time.time()\nwiki.count().compute()\ntime.time() - t0   # 153 sec. using 96 cores on Savio\n\nimport re\ndef find(line, regex = \"Obama\", language = \"en\"):\n    vals = line.split(' ')\n    if len(vals) &lt; 6:\n        return(False)\n    tmp = re.search(regex, vals[3])\n    if tmp is None or (language != None and vals[2] != language):\n        return(False)\n    else:\n        return(True)\n    \n\nwiki.filter(find).count().compute()\n# obama = wiki.filter(find).compute()\n# obama[0:5]\n\nAlternatively, we can use dask-ssh, but I’ve had problems sometimes with using SSH to connect between nodes of a Slurm job, so the approach above is likely to be more robust as it relies on Slurm itself to connect between nodes.\nexport SCHED=$(hostname)\nsrun hostname &gt; .hosts\ndask-ssh --scheduler ${SCHED} --hostfile .hosts",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#effective-parallelization-and-common-issues",
    "href": "python-dask.html#effective-parallelization-and-common-issues",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "6. Effective parallelization and common issues",
    "text": "6. Effective parallelization and common issues\n\n6.1. Nested parallelization and pipelines\nWe can set up nested parallelization (or an arbitrary set of computations) and just have Dask’s delayed functionality figure out how to do the parallelization, provided there is a single call to the compute() method.\n\nimport time, dask.multiprocessing\ndask.config.set(scheduler = 'processes', num_workers = 4)  \n\n@dask.delayed\ndef calc_mean_vargs2(inputs, nobs):\n    import numpy as np\n    rng = np.random.default_rng()\n    data = rng.normal(inputs[0], inputs[1], nobs)\n    return([np.mean(data), np.std(data)])\n\nparams = zip([0,0,1,1],[1,2,1,2])\nm = 20\nn = 10000000\nout = list()\nfor param in params:\n    out_single_param = list()\n    for i in range(m):\n        out_single_param.append(calc_mean_vargs2(param, n))\n    out.append(out_single_param)\n\nt0 = time.time()\noutput = dask.compute(out)  # 7 sec. on 4 cores\ntime.time() - t0\n\n\n\n6.2. Load-balancing and static vs. dynamic task allocation\nWhen using delayed, Dask starts up each delayed evaluation separately (i.e., dynamic allocation). This is good for load-balancing, but each task induces some overhead (a few hundred microseconds).\nEven with a distributed map() it doesn’t appear possible to ask that the tasks be broken up into batches.\nSo if you have many quick tasks, you probably want to break them up into batches manually, to reduce the impact of the overhead.\n\n\n6.3. Avoid repeated calculations by embedding tasks within one call to compute\nAs far as I can tell, Dask avoids keeping all the pieces of a distributed object or computation in memory. However, in many cases this can mean repeating computations or re-reading data if you need to do multiple operations on a dataset.\nFor example, if you are create a Dask distributed dataset from data on disk, I think this means that every distinct set of computations (each computational graph) will involve reading the data from disk again.\nOne implication is that if you can include all computations on a large dataset within a single computational graph (i.e., a call to compute) that may be much more efficient than making separate calls.\nHere’s an example with Dask dataframe on the airline delay data, where we make sure to do all our computations as part of one graph:\n\nimport dask\ndask.config.set(scheduler='processes', num_workers = 6)  \nimport dask.dataframe as ddf\nair = ddf.read_csv('AirlineData/csvs/*.csv.bz2',\n      compression = 'bz2',\n      encoding = 'latin1',   # (unexpected) latin1 value(s) 2001 file TailNum field\n      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n      'TailNum': 'object', 'CancellationCode': 'object'})\n# specify dtypes so Pandas doesn't complain about column type heterogeneity\n\nimport time\nt0 = time.time()\nair.DepDelay.min().compute()   # about 200 seconds.\nt1 = time.time()-t0\nt0 = time.time()\nair.DepDelay.max().compute()   # about 200 seconds.\nt2 = time.time()-t0\nt0 = time.time()\n(mn, mx) = dask.compute(air.DepDelay.max(), air.DepDelay.min())  # about 200 seconds\nt3 = time.time()-t0\nprint(t1)\nprint(t2)\nprint(t3)\n\nHowever, I also tried the above where I added air.count() to the dask.compute call and something went wrong - the computation time increased a lot and there was a lot of memory use. I’m not sure what is going on.\nNote that when reading from disk, disk caching by the operating system (saving files that are used repeatedly in memory) can also greatly speed up I/O. (Note this can very easily confuse you in terms of timing your code…, e.g., simply copying the data to your machine can put them in the cache, so subsequent reading into Python can take advantage of that.)\n\n\n6.4. Copies are usually made\nExcept for the ‘threads’ scheduler, copies will be made of all objects passed to the workers.\nIn general you want to delay the input objects. There are a couple reasons why:\n\nDask hashes the object to create a name, and if you pass the same object as an argument multiple times, it will repeat that hashing.\nWhen using the distributed scheduler only, delaying the inputs will prevent sending the data separately for every task (rather it should send the data separately for each worker).\n\nIn this example, most of the “computational” time is actually spent transferring the data rather than computing the mean. Whether there is a copy per task or a copy per process seems to depend on exactly what the parallelized code is doing (perhaps based on whether there is random number generation in the code).\n\ndask.config.set(scheduler = 'processes', num_workers = 4)  \n\nimport numpy as np\nrng = np.random.default_rng()\nx = rng.normal(size = 40000000)\nx = dask.delayed(x)   # here we delay the data\n\ndef calc(x, i):\n    return(np.mean(x))\n\nout = [dask.delayed(calc)(x, i) for i in range(20)]\nt0 = time.time()\noutput = dask.compute(out)\ntime.time() - t0   # about 20 sec. = 80 total sec. across 4 workers, so ~4 sec. per task\n\n\n## Actual computation is much faster than 4 sec. per task\nt0 = time.time()\ncalc(x, 1)\ntime.time() - t0\n\nHere’s an example of using the Distributed (local) scheduler. Copies have to be made, but if we delay the data object, there is only one copy per worker. Note that with the Dask distributed scheduler, it is complicated to assess what is going on, because the scheduler seems to try to optimize assignment of tasks to workers in a way that may cause an imbalance in the number of tasks assigned to each worker. In this example, all the tasks are assigned to a single worker.\n\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = 4)\nc = Client(cluster)\n\nimport numpy as np\nrng = np.random.default_rng()\nx = rng.normal(size = 40000000)\nx = dask.delayed(x)  # here we delay the data\n\ndef calc(x, i):\n    return(np.mean(x))\n\nout = [dask.delayed(calc)(x, i) for i in range(20)]\nt0 = time.time()\noutput = dask.compute(out)\ntime.time() - t0    # 3.6 sec. \n\nThat took a few seconds if we delay the data but takes ~20 seconds if we don’t.\nAlso, Dask gives a warning about sending the data to the workers in advance. I’m not sure of the distinction between what it is recommending and use of dask.delayed(x). When I tried to use scatter() in various ways, I wasn’t able to silence the warning.\nNote that in either case, we incur the memory usage of the original ‘x’ plus copies of ‘x’ on the workers.\n\n\n6.5. Parallel I/O\nFor this to make the most sense we want to be on a system where we can read multiple files without having the bottleneck of accessing a single spinning hard disk. For example the UC Berkeley Savio cluster filesystem is set up for fast parallel I/O.\nOn systems with a single spinning hard disk or a single SSD, you might experiment to see how effectively things scale as you read (or write) multiple files in parallel.\nHere we’ll demo this. Ideally you would do this on a system with fast, parallel I/O.\n\nimport dask.multiprocessing\ndask.config.set(scheduler = 'processes', num_workers = 24) \n\n\n## define a function that reads data but doesn't need to return entire\n## dataset back to master process to avoid copying cost\ndef readfun(yr):\n    import pandas as pd\n    out = pd.read_csv('airline/' + str(yr) + '.csv.bz2',\n                      header = 0, encoding = 'latin1',\n                      dtype = {'Distance': 'float64', 'CRSElapsedTime': 'float64',\n                      'TailNum': 'object', 'CancellationCode': 'object'})\n                      # specify dtypes so Pandas doesn't complain about column type heterogeneity\n    return(len(out))   # just return length\n\nresults = []\nfor yr in range(1988, 2009):  \n    results.append(dask.delayed(readfun)(yr))\n\n\nimport time\nt0 = time.time()\noutput = dask.compute(results)  # parallel I/O\ntime.time() - t0   ## 120 seconds for 21 files\n\n## Contrast with the time to read a single file:\nt0 = time.time()\nreadfun(1988)\ntime.time() - t0   ## 28 seconds for one file\n\nI’m not sure why that didn’t scale perfectly (i.e., that 21 files on 21 or more workers would take only 28 seconds), but we do see that it was quite a bit faster than sequentially reading the data would be.\n\n\n6.6. Adaptive scaling\nWith a resource manager like Kubernetes, Dask can scale the number of workers up and down to adapt to the computational needs of a workflow. Similarly, if submitting jobs to Slurm via Dask, it will scale up and down automatically - see Section 9.",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#monitoring-jobs",
    "href": "python-dask.html#monitoring-jobs",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "7. Monitoring jobs",
    "text": "7. Monitoring jobs\nDask distributed provides a web interface showing the status of your work. (Unfortunately I’ve been having some problems viewing the interface on SCF machines, but hopefully this will work for you.)\nBy default Dask uses port 8787 for the web interface.\n\nfrom dask.distributed import Client, LocalCluster\ncluster = LocalCluster(n_workers = 4)\nc = Client(cluster)\n\n## open a browser to localhost:8787, then watch the progress\n## as the computation proceeds\n\nn = 100000000\np = 40\n\nfutures = [dask.delayed(calc_mean)(i, n) for i in range(p)]\nt0 = time.time()\nresults = dask.compute(futures)\ntime.time() - t0\n\nIf your Python session is not running on your local machine, you can set up port forwarding to view the web interface in the browser on your local machine, e.g.,\nssh -L 8787:localhost:8787 name_of_remote_machine\nThen go to localhost:8787 in your local browser.",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#reliable-random-number-generation-rng",
    "href": "python-dask.html#reliable-random-number-generation-rng",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "8. Reliable random number generation (RNG)",
    "text": "8. Reliable random number generation (RNG)\nIn the code above, I was cavalier about the seeds for the random number generation in the different parallel computations.\nThe general problem is that we want the random numbers generated on each worker to not overlap with the random numbers generated on other workers. But random number generation involves numbers from a periodic sequence. Simply setting different seeds on different workers does not guarantee non-overlapping blocks of random numbers (though in most cases they probably would not overlap).\nUsing the basic numpy RNG, one can simply set different seeds for each task, but as mentioned above that doesn’t guarantee non-overlapping random numbers.\nWe can use functionality with numpy’s PCG64 or MT19937 generators to be completely safe in our parallel random number generation. Each provide a jumped() function that moves the RNG ahead as if one had generated a very large number of random variables (\\(2^{128}\\) for the Mersenne Twister and nearly that for the PCG64).\nHere’s how we can set up the use of the PCG64 generator:\n\nbitGen = np.random.PCG64(1)\nrng = np.random.Generator(bitGen)\nrng.random(size = 3)\n\nNow let’s see how to jump forward. And then verify that jumping forward two increments is the same as making two separate jumps.\n\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(2)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\nbitGen = np.random.PCG64(1)\nbitGen = bitGen.jumped(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\nWe can also use jumped() with the Mersenne Twister.\n\nbitGen = np.random.MT19937(1)\nbitGen = bitGen.jumped(1)\nrng = np.random.Generator(bitGen)\nrng.normal(size = 3)\n\nSo the strategy to parallelize across tasks (or potentially workers if random number generation is done sequentially for tasks done by a single worker) is to give each task the same seed and use jumped(i) where i indexes the tasks (or workers).\n\ndef myrandomfun(i):\n    bitGen = np.random.PCG(1)\n    bitGen = bitGen.jumped(i)\n    # insert code with random number generation\n\nOne caution is that it appears that the period for PCG64 is \\(2^{128}\\) and that jumped(1) jumps forward by nearly that many random numbers. That seems quite strange, and I don’t understand it.\nAlternatively as recommended in the docs:\n\nn_tasks = 10\nsg = np.random.SeedSequence(1)\nrngs = [Generator(PCG64(s)) for s in sg.spawn(n_tasks)]\n## Now pass elements of `rngs` into your function that is being computed in parallel\n\ndef myrandomfun(rng):\n    # insert code with random number generation, such as:\n    z = rng.normal(size = 5)",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "python-dask.html#submitting-slurm-jobs-from-dask",
    "href": "python-dask.html#submitting-slurm-jobs-from-dask",
    "title": "Parallel processing using the Dask packge in Python",
    "section": "9. Submitting Slurm jobs from Dask",
    "text": "9. Submitting Slurm jobs from Dask\nOne can submit jobs to a scheduler such as Slurm from within Python. In general I don’t recommend this as it requires you to be running Python within a stand-alone server while the Slurm job is running, but here is how one can do it.\nNote that the SLURMCluster() call defines the parameters of a single Slurm job, but scale() is what starts one or more jobs. If you ask for more workers than there are processes defined in your job definition, more than one Slurm job will be launched.\nBe careful to request as many processes as cores; if you leave out processes, it will assume only one Python process (i.e., Dask worker) per job. (Also the memory argument is required.)\nThe queue argument is the Slurm partition you want to submit to.\n\nimport dask_jobqueue\n## Each job will include 16 Dask workers and a total of 16 cores (1 per worker)\ncluster = dask_jobqueue.SLURMCluster(processes=16, cores=16, memory = '24GB',\n                                     queue='low', walltime = '3:00:00')\n\n## The following will now start 32 Dask workers in job(s) on the 'low' partition.\n## In this case, that requires two Slurm jobs of 16 workers each.\ncluster.scale(32)  \nfrom dask.distributed import Client\nc = Client(cluster)\n\n## carry out your parallel computations\n\ncluster.close()\n\nDask is requiring the specification of ‘memory’ though this is not always required by the underlying cluster.\nOn a cluster like Savio where you may need to provide an account (-A flag), you pass that via the project argument to SLURMCluster.\nIf you’d like to see the Slurm job script that Dask constructs and submits, you can run:\n\ncluster.job_script()\n\n\n9.1. Adaptive scaling\nIf you use cluster.adapt() in place of cluster.scale(), Dask will start and stop Slurm jobs to start and stop workers as needed. Note that on a shared cluster, you will almost certainly want to set a maximum number of workers to run at once so you don’t accidentally submit 100s or 1000s of jobs.\nI’m still figuring out how this works. It seems to work well when having each Slurm job control one worker on one core - in that case Dask starts a set of workers and uses those workers to iterate through the tasks. However when I try to use 16 workers per Slurm job, Dask submits a series of single 16-core jobs rather than using two 16-core jobs that stay active while working through the tasks.\n\nimport dask_jobqueue\n## Each job will include 16 Dask workers and a total of 16 cores (1 per worker)\n## This should now start up to 32 Dask workers, but only one set of 16 workers seems to start\n## and Slurm jobs start and stop in quick succession.\n## cluster = dask_jobqueue.SLURMCluster(processes=16, cores=16, memory='24 GB',\n## queue='low', walltime = '3:00:00')  \n\n## In contrast, this seems to work well.\ncluster = dask_jobqueue.SLURMCluster(cores = 1, memory = '24 GB',\n                                     queue='low', walltime = '3:00:00')\n\ncluster.adapt(minimum=0, maximum=32)\n\nfrom dask.distributed import Client\nc = Client(cluster)\n\n## carry out your parallel computations\np=200\nn=10000000\ninputs = [(i, n) for i in range(p)]\n# execute the function across the array of input values\nfuture = c.map(calc_mean_vargs, inputs)\nresults = c.gather(future)\n\ncluster.close()",
    "crumbs": [
      "Dask in Python"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Flexible parallel processing using the R future and Python Dask packages",
    "section": "",
    "text": "This tutorial covers the use of R’s future and Python’s Dask packages, well-established tools for parallelizing computions on a single machine or across multiple machines. There is also a bit of material on Python’s Ray package, which was developed more recently (but has been around for a while).\nYou should be able to replicate much of what is covered here provided you have Rand Python on your computer, but some of the parallelization approaches may not work on Windows.\nThis tutorial assumes you have a working knowledge of either R or Python, but not necessarily knowledge of parallelization in R or Python.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#this-tutorial",
    "href": "index.html#this-tutorial",
    "title": "Flexible parallel processing using the R future and Python Dask packages",
    "section": "",
    "text": "This tutorial covers the use of R’s future and Python’s Dask packages, well-established tools for parallelizing computions on a single machine or across multiple machines. There is also a bit of material on Python’s Ray package, which was developed more recently (but has been around for a while).\nYou should be able to replicate much of what is covered here provided you have Rand Python on your computer, but some of the parallelization approaches may not work on Windows.\nThis tutorial assumes you have a working knowledge of either R or Python, but not necessarily knowledge of parallelization in R or Python.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#some-useful-terminology",
    "href": "index.html#some-useful-terminology",
    "title": "Flexible parallel processing using the R future and Python Dask packages",
    "section": "2. Some useful terminology",
    "text": "2. Some useful terminology\n\ncores: We’ll use this term to mean the different processing units available on a single node.\nnodes: We’ll use this term to mean the different computers, each with their own distinct memory, that make up a cluster or supercomputer.\nprocesses: instances of a program executing on a machine; multiple processes may be executing at once. A given executable (e.g., Python or R) may start up multiple processes at once. Ideally we have no more user processes than cores on a node.\nthreads: multiple paths of execution within a single process; the OS sees the threads as a single process, but one can think of them as ‘lightweight’ processes. Ideally when considering the processes and their threads, we would have the number of total threads across all processes not exceed the number of cores on a node.\nforking: child processes are spawned that are identical to the parent, but with different process IDs and their own memory.\nsockets: some of R’s parallel functionality involves creating new R processes (e.g., starting processes via Rscript) and communicating with them via a communication technology called sockets.\ntasks: This term gets used in various ways (including in place of ‘processes’), but we’ll use it to refer to the individual computational items you want to complete - e.g., one task per cross-validation fold or one task per simulation replicate/iteration.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#types-of-parallel-processing",
    "href": "index.html#types-of-parallel-processing",
    "title": "Flexible parallel processing using the R future and Python Dask packages",
    "section": "3. Types of parallel processing",
    "text": "3. Types of parallel processing\nThere are two basic flavors of parallel processing (leaving aside GPUs): distributed memory and shared memory. With shared memory, multiple processors (which I’ll call cores) share the same memory. With distributed memory, you have multiple nodes, each with their own memory. You can think of each node as a separate computer connected by a fast network.\n\n3.1. Shared memory\nFor shared memory parallelism, each core is accessing the same memory so there is no need to pass information (in the form of messages) between different machines. But in some programming contexts one needs to be careful that activity on different cores doesn’t mistakenly overwrite places in memory that are used by other cores.\nHowever, except for certain special situations, the different worker processes on a given machine do not share objects in memory. So most often, one has multiple copies of the same objects, one per worker process.\n\nThreading\nThreads are multiple paths of execution within a single process. If you are monitoring CPU usage (such as with top in Linux or Mac) and watching a job that is executing threaded code, you’ll see the process using more than 100% of CPU. When this occurs, the process is using multiple cores, although it appears as a single process rather than as multiple processes.\nNote that this is a different notion than a processor that is hyperthreaded. With hyperthreading a single core appears as two cores to the operating system.\nThreads generally do share objects in memory, thereby allowing us to have a single copy of objects instead of one per thread.\n\n\n\n3.2. Distributed memory\nParallel programming for distributed memory parallelism requires passing messages between the different nodes.\nThe standard protocol for passing messages is MPI, of which there are various versions, including openMPI.\nTools such as Dask, Ray and future all manage the work of moving information between nodes for you (and don’t generally use MPI).\n\n\n3.3. Other type of parallel processing\nWe won’t cover either of these in this material.\n\nGPUs\nGPUs (Graphics Processing Units) are processing units originally designed for rendering graphics on a computer quickly. This is done by having a large number of simple processing units for massively parallel calculation. The idea of general purpose GPU (GPGPU) computing is to exploit this capability for general computation.\nMost researchers don’t program for a GPU directly but rather use software (often machine learning software such as PyTorch, JAX, or Tensorflow) that has been programmed to take advantage of a GPU if one is available.\n\n\nSpark and Hadoop\nSpark and Hadoop are systems for implementing computations in a distributed memory environment, using the MapReduce approach.\nNote that Dask provides a lot of the same functionality as Spark, allowing one to create distributed datasets where pieces of the dataset live on different machines but can be treated as a single dataset from the perspective of the user.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "index.html#package-comparison",
    "href": "index.html#package-comparison",
    "title": "Flexible parallel processing using the R future and Python Dask packages",
    "section": "4. Package comparison",
    "text": "4. Package comparison\nSee this outline for an overview comparison of future ( R ), Dask (Python) and Ray (Python)\n\nUse cases\n\nfuture\n\nparallelizing tasks\none or more machines (nodes)\n\nDask\n\nparallelizing tasks\ndistributed datasets\none or more machines (nodes)\n\nRay\n\nparallelizing tasks\nbuilding distributed applications (interacting tasks)\none or more machines (nodes)\n\n\nTask allocation\n\nfuture\n\nstatic by default\ndynamic optional\n\nDask\n\ndynamic\n\nRay\n\ndynamic (I think)\n\n\nShared memory\n\nfuture\n\nshared across workers only if using forked processes (multicore plan on Linux/MacOS)\n\nif data modified, copies need to be made\n\ndata shared across static tasks assigned to a worker\n\nDask\n\nshared across workers only if using threads scheduler\ndata shared across tasks on a worker if data are delayed\n\nRay\n\nshared across all workers on a node via the object store (nice!)\n\n\nCopies made\n\nfuture\n\ngenerally copy one per worker, but one per task with dynamic allocation\n\nDask\n\ngenerally one copy per worker if done correctly (data should be delayed if using distributed scheduler)\n\nRay\n\none copy per node if done correctly (use ray.put to use object store)",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "License"
    ]
  }
]