<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" type="text/css" media="screen, projection, print"
    href="http://www.w3.org/Talks/Tools/Slidy2/styles/slidy.css" />
  <script src="http://www.w3.org/Talks/Tools/Slidy2/scripts/slidy.js"
    charset="utf-8" type="text/javascript"></script>
</head>
<body>
<div id="parallel-processing-using-the-r-future-package" class="slide section level1">
<h1>Parallel Processing using the R future package</h1>
<p>Chris Paciorek, Department of Statistics, UC Berkeley</p>
</div>
<div id="overview-futures-and-the-r-future-package" class="slide section level1">
<h1>1. Overview: Futures and the R future package</h1>
<p>What is a future? It's basically a flag used to tag a given operation such that when and where that operation is carried out is controlled at a higher level. If there are multiple operations tagged then this allows for parallelization across those operations.</p>
<p>According to Henrik Bengtsson (the package developer) and those who developed the concept:</p>
<ul>
<li>a future is an abstraction for a value that will be available later</li>
<li>the value is the result of an evaluated expression</li>
<li>the state of a future is either unresolved or resolved</li>
</ul>
</div>
<div id="why-use-futures" class="slide section level1">
<h1>1.1. Why use futures?</h1>
<p>The future package allows one to write one's computational code without hard-coding whether or how parallelization would be done. Instead one writes the code in a generic way and at the top of one's code sets the 'plan' for how the parallel computation should be done given the computational resources available. Simply changing the 'plan' changes how parallelization is done for any given run of the code.</p>
<p>More concisely, the key ideas are:</p>
<ul>
<li>Separate what to parallelize from how and where the parallelization is actually carried out.</li>
<li>Different users can run the same code on different computational resources (without touching the actual code that does the computation).</li>
</ul>
</div>
<div id="overview-of-parallel-backends" class="slide section level1">
<h1>2. Overview of parallel backends</h1>
<p>One uses <code>plan()</code> to control how parallelization is done, including what machine(s) to use and how many cores on each machine to use.</p>
<p>For example,</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(multiprocess)  ## spreads work across multiple cores
<span class="co"># also, can control number of workers</span>
<span class="kw">plan</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">4</span>)</code></pre></div>
<p>This table gives an overview of the different plans.</p>
<table style="width:100%;">
<colgroup>
<col width="12%" />
<col width="55%" />
<col width="8%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="left">Description</th>
<th align="left">Multi-node</th>
<th align="left">Copies of objects made?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">multiprocess</td>
<td align="left">either multicore (non-Windows) or multisession (Windows)</td>
<td align="left">no</td>
<td align="left">see below</td>
</tr>
<tr class="even">
<td align="left">multisession</td>
<td align="left">background R sessions</td>
<td align="left">no</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="left">multicore</td>
<td align="left">forked R processes</td>
<td align="left">no</td>
<td align="left">not if object not modified</td>
</tr>
<tr class="even">
<td align="left">remote</td>
<td align="left">R process on another machine</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
<tr class="odd">
<td align="left">cluster</td>
<td align="left">R sessions on other machine(s)</td>
<td align="left">yes</td>
<td align="left">yes</td>
</tr>
</tbody>
</table>
<p>For the next section (Section 3), we'll just assume use of multiprocess and will provide more details on the other plans in the following section (Section 4).</p>
</div>
<div id="implementing-operations-in-parallel" class="slide section level1">
<h1>3. Implementing operations in parallel</h1>
<p>The future package has a few main patterns for how you might parallelize a computation.</p>
</div>
<div id="parallelized-lapply-statements-and-related" class="slide section level1">
<h1>3.1. Parallelized lapply statements and related</h1>
<p>You can parallelize lapply and related functions easily. This is a nice replacement for the confusingly similar set of such as <code>parLapply</code>, <code>mclapply</code>, and <code>mpi.parSapply</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(future.apply)
<span class="kw">plan</span>(multiprocess)  <span class="co"># or some other plan</span>
output &lt;-<span class="st"> </span><span class="kw">future_lapply</span>(<span class="dv">1</span>:<span class="dv">20</span>, function(i) <span class="kw">mean</span>(<span class="kw">rnorm</span>(<span class="fl">1e7</span>)), <span class="dt">future.seed =</span> <span class="dv">1</span>)
<span class="co"># or sapply:</span>
<span class="co"># output &lt;- future_sapply(1:20, function(i) mean(rnorm(1e7)), future.seed = 1)</span></code></pre></div>
</div>
<div id="foreach" class="slide section level1">
<h1>3.2. foreach</h1>
<p>You can also continue to use <code>foreach</code> if you like that approach. (Note the code here is not safe in terms of parallel randon number generation - see section later in this document.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(multiprocess)  <span class="co"># or some other plan</span>

<span class="kw">library</span>(doFuture)</code></pre></div>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">registerDoFuture</span>()

out &lt;-<span class="st"> </span><span class="kw">foreach</span>(<span class="dt">i =</span> <span class="dv">1</span>:<span class="dv">20</span>) %dopar%<span class="st"> </span>{
    <span class="kw">print</span>(i)
    <span class="kw">mean</span>(<span class="kw">rnorm</span>(<span class="fl">1e7</span>))
}</code></pre></div>
<pre><code>## [1] 1
## [1] 2
## [1] 3
## [1] 4
## [1] 5
## [1] 6
## [1] 7
## [1] 8
## [1] 9
## [1] 10
## [1] 11
## [1] 12
## [1] 13
## [1] 14
## [1] 15
## [1] 16
## [1] 17
## [1] 18
## [1] 19
## [1] 20</code></pre>
<p>Note that unlike with <code>doParallel</code>, the delay in printing the iteration values may indicate that tasks are preallocated and dispatched to the workers at once. I haven't investigated this.</p>
</div>
<div id="using-futures-for-parallelization" class="slide section level1">
<h1>3.3. Using futures for parallelization</h1>
<p>While future_lapply and foreach are fine, the future package introduces a new style of parallelizing code using an explicit &quot;future&quot;. Here we include the code for individual iterations inside <code>future()</code> to mark the unit of computation. The future package will then distribute the individual iterations to run in parallel, based on the plan.</p>
<p>(Note the code here is not safe in terms of parallel randon number generation - see section later in this document.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">4</span>)   <span class="co"># or some other plan</span>
n &lt;-<span class="st"> </span><span class="dv">20</span>
out &lt;-<span class="st"> </span><span class="kw">list</span>(); <span class="kw">length</span>(out) &lt;-<span class="st"> </span>n
for(i in <span class="kw">seq_len</span>(n)) {
     out[[i]] &lt;-<span class="st"> </span><span class="kw">future</span>( {
       ## simply insert code here as you would with foreach; for example:
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
     })
}
<span class="kw">class</span>(out[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>## [1] &quot;MulticoreFuture&quot;    &quot;MultiprocessFuture&quot; &quot;Future&quot;            
## [4] &quot;environment&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Each return values (e.g., &#39;out[[1]]&#39;) is a wrapper, so use value() to access:
<span class="kw">value</span>(out[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>## [1] 0.0003627099 1.0000119088</code></pre>
</div>
<div id="using-implicit-futures-with-listenvs" class="slide section level1">
<h1>3.4. Using implicit futures (with listenvs)</h1>
<p>In addition to using <code>future()</code>, one can use the special <code>%&lt;-%</code> operator to denote a future. The <code>%&lt;-%</code> operator can only operate with an environment. So we create a <code>listenv</code>, which is basically an environment that can be treated like a list.</p>
<p>This approach creates implicit futures, and one does not need to use <code>value</code> to get the result.</p>
<p>(Note the code here is not safe in terms of parallel randon number generation - see section later in this document.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(listenv)

<span class="kw">plan</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">4</span>)
n &lt;-<span class="st"> </span><span class="dv">20</span>
out &lt;-<span class="st"> </span><span class="kw">listenv</span>()
for(i in <span class="kw">seq_len</span>(n)) {
     out[[i]] %&lt;-%<span class="st"> </span>{
       <span class="co"># some code here as you would with foreach</span>
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
      }
}

out[[<span class="dv">2</span>]]</code></pre></div>
<pre><code>## [1] 0.0004315569 1.0003901878</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out</code></pre></div>
<pre><code>## A &#39;listenv&#39; vector with 20 elements (unnamed).</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">out &lt;-<span class="st"> </span><span class="kw">as.list</span>(out)</code></pre></div>
</div>
<div id="blocking-and-non-blocking-calls" class="slide section level1">
<h1>3.5. Blocking and non-blocking calls</h1>
<p>A 'blocking call' prevents the user from continuing to evaluate more expressions. Often, futures are evaluated in an asynchronous way and therefore are non-blocking except for when the actual evaluated value of the expression is requested.</p>
<p>Here we see that control returns to the user right away but asking for the value of the expression is a blocking call.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># future() is non-blocking</span>
<span class="kw">system.time</span>(
     out &lt;-<span class="st"> </span><span class="kw">future</span>( {
       ## some code here as in foreach
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">2e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
       })
)</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.004   0.004   0.008</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># getting the value is blocking</span>
<span class="kw">system.time</span>(<span class="kw">value</span>(out))</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.944   0.028   1.699</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># %&lt;-% is non-blocking</span>
<span class="kw">system.time</span>(
     out %&lt;-%<span class="st"> </span>{
       ## some code here as in foreach
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">2e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
       })</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.004   0.004   0.006</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># getting the value is blocking</span>
<span class="kw">system.time</span>(out)</code></pre></div>
<pre><code>##    user  system elapsed 
##   1.704   0.072   1.703</code></pre>
<h3 id="blocking-in-the-context-of-a-loop-over-futures">Blocking in the context of a loop over futures</h3>
<p>In contrast, in a for loop, creation of additional futures is blocked if all workers are busy evaluating other futures. So in this case, evaluation of the first four futures blocks, but once the last two futures start to be evaluated, control returns to the user while those futures are evaluated in the background.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">2</span>)
n &lt;-<span class="st"> </span><span class="dv">6</span>
out &lt;-<span class="st"> </span><span class="kw">listenv</span>()
<span class="kw">system.time</span>(<span class="kw">rnorm</span>(<span class="fl">2e7</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##   1.608   0.040   1.645</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## blocks until all futures are dispatched, so this should
## take twice the time it takes for rnorm(2e7) to occur
<span class="kw">system.time</span>(
for(i in <span class="kw">seq_len</span>(n)) {
     out[[i]] %&lt;-%<span class="st"> </span>{
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">2e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
     }})</code></pre></div>
<pre><code>##    user  system elapsed 
##   7.224   0.296   4.502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## not blocked as result already available
<span class="kw">system.time</span>(out[[<span class="dv">2</span>]])</code></pre></div>
<pre><code>##    user  system elapsed 
##       0       0       0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## not blocked as result already available
<span class="kw">system.time</span>(out[[<span class="dv">4</span>]])</code></pre></div>
<pre><code>##    user  system elapsed 
##       0       0       0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## blocked as result still being evaluated
<span class="kw">system.time</span>(out[[<span class="dv">6</span>]])</code></pre></div>
<pre><code>##    user  system elapsed 
##   1.896   0.064   1.671</code></pre>
</div>
<div id="a-tour-of-different-backends" class="slide section level1">
<h1>4. A tour of different backends</h1>
</div>
<div id="serial-sequential-processing" class="slide section level1">
<h1>4.1. Serial (sequential) processing</h1>
<p>The <code>sequential</code> plan allows you to run code on a single local core. This might not seem all that useful since the goal is usually to parallelize, but this helps in debugging and allows someone to run future-based code even if they only have one core available.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(sequential)
## future_lapply, foreach with doFuture, etc. all will still work</code></pre></div>
<p>Actually even better for debugging is the <code>transparent</code> plan, which provides additional useful output.</p>
</div>
<div id="multicore-processing-on-one-machine" class="slide section level1">
<h1>4.2. Multicore processing on one machine</h1>
<p>We've already seen that we can use the <code>multiprocess</code> plan to parallelize across the cores of one machine.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">2</span>)</code></pre></div>
</div>
<div id="distributed-processing-across-multiple-machines-via-an-ad-hoc-cluster" class="slide section level1">
<h1>4.3. Distributed processing across multiple machines via an ad hoc cluster</h1>
<p>If we know the names of the machines and can access them via password-less SSH, then we can simply provide the names of the machines to create a cluster and use the 'cluster' plan.</p>
<p>Here we want to use four cores on one machine.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">workers &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="st">&#39;arwen&#39;</span>,<span class="dv">4</span>)
<span class="kw">plan</span>(cluster, <span class="dt">workers =</span> workers)</code></pre></div>
<p>Here we want to use two cores on one machine and two on another.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">workers &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;arwen&#39;</span>, <span class="dv">2</span>), <span class="kw">rep</span>(<span class="st">&#39;beren&#39;</span>, <span class="dv">2</span>))
<span class="kw">plan</span>(cluster, <span class="dt">workers =</span> workers)
<span class="co"># Check we are getting workers in the right places:</span>
<span class="kw">future_sapply</span>(<span class="kw">seq_along</span>(workers), function(i) <span class="kw">Sys.getenv</span>(<span class="st">&#39;HOST&#39;</span>))</code></pre></div>
<pre><code>## [1] &quot;arwen&quot; &quot;arwen&quot; &quot;beren&quot; &quot;beren&quot;</code></pre>
</div>
<div id="distributed-processing-across-multiple-machines-within-a-slurm-scheduler-job" class="slide section level1">
<h1>4.4. Distributed processing across multiple machines within a SLURM scheduler job</h1>
<p>If you are using SLURM and in your sbatch or srun command you use <code>--ntasks</code>, then the following will allow you to use as many R workers as the value of <code>ntasks</code>. One caveat is that one still needs to be able to access the various machines via password-less SSH, which is we are just now making possible on the SCF cluster (and already works on Savio).</p>
<p>(Ignore the &quot;srun: error&quot; that may occur - that's a non-consequential bug in SLURM.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">workers &lt;-<span class="st"> </span><span class="kw">system</span>(<span class="st">&#39;srun hostname&#39;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>)
cl &lt;-<span class="st"> </span>parallel::<span class="kw">makeCluster</span>(workers)
<span class="kw">plan</span>(cluster, <span class="dt">workers =</span> cl)
<span class="co"># and verify we&#39;re actually connected to the workers:</span>
<span class="kw">future_sapply</span>(<span class="kw">seq_along</span>(workers), function(i) <span class="kw">system</span>(<span class="st">&#39;hostname&#39;</span>, <span class="dt">intern =</span> <span class="ot">TRUE</span>))</code></pre></div>
<p>That works because <code>srun hostname</code> will print out the node IDs associated with each task within a SLURM job.</p>
<p>Note that for this to work on Savio with multiple nodes, you will probably need to load the R module via your .bashrc so that all the nodes have R and dependent modules available.</p>
<p>One can also directly pass the vector of worker names to the <code>workers</code> argument of <code>plan()</code>, which should invoke <code>future::makeClusterPSOCK</code>, but I was having trouble with that hanging on Savio.</p>
</div>
<div id="off-loading-work-to-another-machine" class="slide section level1">
<h1>4.5. Off-loading work to another machine</h1>
<p>One can run a chunk of code on a remote machine, for example if you need a machine with more memory.</p>
<p>Here's an example where I create a plot remotely and view it locally.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(remote, <span class="dt">workers =</span> <span class="st">&#39;gandalf.berkeley.edu&#39;</span>)
## requires password-less SSH

## future is evaluated remotely
<span class="kw">library</span>(ggplot2)
mydf &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">y =</span> <span class="kw">rnorm</span>(<span class="dv">10</span>), <span class="dt">x =</span> <span class="kw">rnorm</span>(<span class="dv">10</span>))
g %&lt;-%<span class="st"> </span>{ <span class="kw">ggplot</span>(mydf, <span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>y)) +<span class="st"> </span><span class="kw">geom_point</span>() }

## plot locally
g</code></pre></div>
<div class="figure">
<img src="figure/off-load-1.png" />

</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">g %&lt;-%<span class="st"> </span>R.devices::<span class="kw">capturePlot</span>({
   <span class="kw">filled.contour</span>(volcano, <span class="dt">color.palette =</span> terrain.colors)
   <span class="kw">title</span>(<span class="dt">main =</span> <span class="st">&quot;volcano data: filled contour map&quot;</span>)
   })         

## plot locally
g</code></pre></div>
<pre><code>## Warning in restoreRecordedPlot(x, reloadPkgs): snapshot recorded in
## different R version (3.5.1)</code></pre>
<div class="figure">
<img src="figure/off-load-2.png" />

</div>
</div>
<div id="load-balancing-and-static-vs.-dynamic-task-allocation" class="slide section level1">
<h1>5. Load-balancing and static vs. dynamic task allocation</h1>
<ul>
<li>future_lapply uses static (non-load-balanced) allocation.
<ul>
<li>The future_lapply functionality by default groups iterations into tasks and creates only as many tasks as there are workers. See the <code>future.scheduling</code> argument for user control over how the allocation is done. This is good for reducing overhead but can potentially result in bad load-balancing if the tasks assigned to one worker take a very different time to complete from those assigned to a different worker.</li>
</ul></li>
<li>explicit or implicit futures use dynamic (load-balanced) allocation
<ul>
<li>involves dispatching one task per iteration, with the resulting overhead</li>
</ul></li>
</ul>
</div>
<div id="avoiding-copies-when-doing-multi-process-parallelization-on-a-single-node" class="slide section level1">
<h1>6. Avoiding copies when doing multi-process parallelization on a single node</h1>
<p>If you're working with large objects, making a copy of the objects for each of the worker processes can be a significant time cost and can greatly increase your memory use.</p>
<p>On non-Windows machines, the multicore plan (which is what is used by the multiprocess plan on such machines) forks the main R process. This creates R worker processes with the same state as the original R process. Interestingly, this means that global variables in the forked worker processes are just references to the objects in memory in the original R process. So the additional processes do not use additional memory for those objects (despite what is shown in <em>top</em>) and there is no time involved in making copies. However, if you modify objects in the worker processes then copies are made.</p>
<p>Unfortunately, the <em>top</em> program on a Linux machine or a Mac will make it look like additional memory is being used. On a Linux machine the command <code>free -h</code> shows the usable remaining memory under the 'Available' column of the 'Mem' row, e.g., here we see 6.5 GB available and 8.2 GB used. (Note that the 'free' column omits memory that is actually available but is temporarily in use for caching.)</p>
<pre><code>paciorek@smeagol:~/staff/workshops/r-future&gt; free -h
              total        used        free      shared  buff/cache   available
Mem:            15G        8.2G        4.3G        453M        3.1G        6.5G
Swap:          7.6G        2.9G        4.7G</code></pre>
<p>Here's some test code you can run while monitoring <code>free -h</code>. E.g., here is a good way to invoke it to see the output in real time.</p>
<div class="sourceCode"><pre class="sourceCode bash"><code class="sourceCode bash"><span class="kw">watch</span> -n 0.1 free -h</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># allow total size of global variables to be large enough...</span>
<span class="kw">options</span>(<span class="dt">future.globals.maxSize =</span> <span class="fl">1e9</span>)
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">5e7</span>)
<span class="kw">plan</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">3</span>)  <span class="co"># forks (where supported, not Windows); no copying!</span>
<span class="kw">system.time</span>(tmp &lt;-<span class="st"> </span><span class="kw">future_sapply</span>(<span class="dv">1</span>:<span class="dv">100</span>, function(i) <span class="kw">mean</span>(x)))

<span class="kw">options</span>(<span class="dt">future.globals.maxSize =</span> <span class="fl">1e9</span>)
x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">5e7</span>)
<span class="kw">plan</span>(multisession, <span class="dt">workers =</span> <span class="dv">3</span>) <span class="co"># new processes - copying!</span>
<span class="kw">system.time</span>(tmp &lt;-<span class="st"> </span><span class="kw">future_sapply</span>(<span class="dv">1</span>:<span class="dv">100</span>, function(i) <span class="kw">mean</span>(x)))</code></pre></div>
</div>
<div id="nested-futuresfor-loops" class="slide section level1">
<h1>7. Nested futures/for loops</h1>
<p>You can set up nested parallelization and use various plans to parallelize at each level.</p>
<p>For example, suppose you are running a simulation study with four scenarios and each scenario involving <code>n</code> simulations. In that case you have two loops over which you could choose to parallelize.</p>
<p>Here's some syntax to setup up parallelization over the scenarios only. Note that when the plan involves multiple levels we need to use <code>tweak</code> if we want to modify the defaults for a type of future.</p>
<p>(Note the code here is not safe in terms of parallel random number generation - see section later in this document.)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(<span class="kw">list</span>(<span class="kw">tweak</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">4</span>), sequential))

params &lt;-<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>))
p &lt;-<span class="st"> </span><span class="kw">nrow</span>(params)  <span class="co"># 4 in this case</span>
n &lt;-<span class="st"> </span><span class="dv">20</span>
out &lt;-<span class="st"> </span><span class="kw">listenv</span>()
for(k in <span class="kw">seq_len</span>(p)) {   <span class="co"># outer loop: parameter sweep</span>
     out[[k]] %&lt;-%<span class="st"> </span>{    
        out_single_param &lt;-<span class="st"> </span><span class="kw">listenv</span>()
        for(i in <span class="kw">seq_len</span>(n)) {   <span class="co"># inner loop: replications</span>
          out_single_param[[i]] %&lt;-%<span class="st"> </span>{
            tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">2e7</span>, params[k, <span class="dv">1</span>], params[k, <span class="dv">2</span>])
            <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
          }
        }
        <span class="kw">matrix</span>(<span class="kw">unlist</span>(out_single_param), <span class="dt">ncol =</span> <span class="dv">2</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)
     }
}
## non-blocking - note that control returns to the user since we have
## four outer iterations and four workers
out</code></pre></div>
<pre><code>## A &#39;listenv&#39; vector with 4 elements (unnamed).</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## asking for an actual value is a blocking call
out[[<span class="dv">1</span>]]</code></pre></div>
<pre><code>##                [,1]      [,2]
##  [1,]  1.197635e-04 0.9999513
##  [2,] -2.943107e-04 0.9998773
##  [3,] -8.055966e-05 1.0000890
##  [4,] -8.873961e-05 1.0001672
##  [5,]  1.994883e-05 1.0004227
##  [6,]  7.477603e-05 1.0001955
##  [7,] -5.449133e-05 0.9997913
##  [8,]  1.136925e-04 1.0000480
##  [9,] -7.128670e-05 1.0000666
## [10,] -4.333254e-04 1.0001005
## [11,]  3.427579e-04 1.0002659
## [12,] -1.972929e-04 1.0002067
## [13,] -5.594750e-04 1.0000001
## [14,]  2.909209e-04 0.9999528
## [15,] -2.735091e-04 1.0000960
## [16,]  2.193245e-04 1.0004056
## [17,]  7.991547e-05 0.9998049
## [18,] -7.701249e-05 1.0001779
## [19,]  6.650595e-05 0.9999232
## [20,]  6.206785e-05 0.9999539</code></pre>
<p>Note that these are &quot;asynchronous&quot; futures that are evaluated in the background while control returns to the user.</p>
</div>
<div id="nested-futuresfor-loops---some-example-plans" class="slide section level1">
<h1>7.1. Nested futures/for loops - some example plans</h1>
<p>Let's see a few different plans one could use for the nested loops.</p>
<p>To use eight cores on the current machine, two cores per outer iteration:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## One option:
<span class="kw">plan</span>(<span class="kw">list</span>(<span class="kw">tweak</span>(multisession, <span class="dt">workers =</span> <span class="dv">4</span>), <span class="kw">tweak</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">2</span>)))
## Another option
nodes &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="st">&#39;localhost&#39;</span>, <span class="dv">4</span>)
<span class="kw">plan</span>(<span class="kw">list</span>(<span class="kw">tweak</span>(cluster, <span class="dt">workers =</span> nodes), <span class="kw">tweak</span>(multiprocess, <span class="dt">workers =</span> <span class="dv">2</span>)))</code></pre></div>
<p>To run each parameter across as many workers as are available on each of multiple machines:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nodes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;arwen&#39;</span>, <span class="st">&#39;beren&#39;</span>, <span class="st">&#39;radagast&#39;</span>, <span class="st">&#39;gandalf&#39;</span>)
<span class="kw">plan</span>(<span class="kw">list</span>(<span class="kw">tweak</span>(cluster, <span class="dt">workers =</span> nodes), multiprocess))</code></pre></div>
<p>If there are many inner iterations and few outer iterations, we might simply do the outer iterations sequentially:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(<span class="kw">list</span>(sequential, multiprocess))</code></pre></div>
<p>Note that you can't use a multicore future at multiple levels (or equivalently multiprocess on a machine where it does multicore) as future prevents nested multicore parallelization:</p>
<p>Once you've set your code to allow parallelization at multiple levels, you can change the plan without ever touching the core code again.</p>
</div>
<div id="hybrid-parallelization-multiple-processes-plus-threaded-linear-algebra" class="slide section level1">
<h1>7.2. Hybrid parallelization: multiple processes plus threaded linear algebra</h1>
<p>If you have access to threaded BLAS (e.g., MKL or openBLAS), as long as you set OMP_NUM_THREADS greater than 1, then any linear algebra should be parallelized within each of the iterations in a loop or apply statement.</p>
</div>
<div id="rng" class="slide section level1">
<h1>8. RNG</h1>
<p>In the code above, I was cavalier about the seeds for the random number generation in the different parallel computations.</p>
<p>The future package integrates well with the L'Ecuyer parallel RNG approach, although the documentation is not great. There is a good discussion about seeds for <code>future_lapply</code> and <code>future_sapply</code> in the help for those functions.</p>
<p>Here's what I have been able to learn.</p>
</div>
<div id="future_lapply" class="slide section level1">
<h1>8.1. future_lapply</h1>
<p>Here we can set a single seed. Behind the scenes the L'Ecuyer-CMRG RNG is used so that the random numbers generated for each iteration are independent. Note there is some overhead here when the number of iterations is large.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(future.apply)
n &lt;-<span class="st"> </span><span class="dv">4</span>
<span class="kw">future_sapply</span>(<span class="dv">1</span>:n, function(i) <span class="kw">rnorm</span>(<span class="dv">1</span>), <span class="dt">future.seed =</span> <span class="dv">1</span>)</code></pre></div>
<pre><code>## [1]  1.3775667 -1.7371292 -0.1362109  1.9301162</code></pre>
<p>Basically future_lapply pregenerates a seed for each iteration using <code>parallel:::nextRNGStream</code>, which uses the L'Ecuyer algorithm. See <a href="https://github.com/HenrikBengtsson/future/issues/126">more details here</a>.</p>
</div>
<div id="using-future" class="slide section level1">
<h1>8.2. using future()</h1>
<p>You can (and should when using RNG) set the seed in <code>future()</code>. You'll need to set it to a L'Ecuyer-CMRG seed, which is a vector of six integers. Here's how you can do it, advancing the seed for each iteration using nextRNGStream().</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plan</span>(multiprocess)   <span class="co"># or some other plan</span>

<span class="kw">RNGkind</span>(<span class="st">&quot;L&#39;Ecuyer-CMRG&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
nextSeed &lt;-<span class="st"> </span>.Random.seed
n &lt;-<span class="st"> </span><span class="dv">10</span>
out &lt;-<span class="st"> </span><span class="kw">list</span>(); <span class="kw">length</span>(out) &lt;-<span class="st"> </span>n
for(i in <span class="kw">seq_len</span>(n)) {
     out[[i]] &lt;-<span class="st"> </span><span class="kw">future</span>( {
       ## some code here as in foreach
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
     }, <span class="dt">seed =</span> nextSeed)
     nextSeed &lt;-<span class="st"> </span>parallel::<span class="kw">nextRNGStream</span>(nextSeed)
}
valsExplicit &lt;-<span class="st"> </span><span class="kw">sapply</span>(out, value)</code></pre></div>
<p>or with implicit futures:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(listenv)
<span class="kw">RNGkind</span>(<span class="st">&quot;L&#39;Ecuyer-CMRG&quot;</span>)
<span class="kw">set.seed</span>(<span class="dv">1</span>)
nextSeed &lt;-<span class="st"> </span>.Random.seed
<span class="kw">plan</span>(multiprocess)   <span class="co"># or some other plan</span>
n &lt;-<span class="st"> </span><span class="dv">10</span>
out &lt;-<span class="st"> </span><span class="kw">listenv</span>()
for(i in <span class="kw">seq_len</span>(n)) {
     out[[i]] %&lt;-%<span class="st"> </span>{
       ## some code here as in foreach
       tmp &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e7</span>)
       <span class="kw">c</span>(<span class="kw">mean</span>(tmp), <span class="kw">sd</span>(tmp))
     } %seed%<span class="st"> </span>nextSeed
     nextSeed &lt;-<span class="st"> </span>parallel::<span class="kw">nextRNGStream</span>(nextSeed)
}
<span class="kw">as.list</span>(out)</code></pre></div>
<pre><code>## [[1]]
## [1] -0.0003724317  1.0001297117
## 
## [[2]]
## [1] -0.0002463121  0.9997807864
## 
## [[3]]
## [1] -0.0001795439  0.9997580553
## 
## [[4]]
## [1] -0.000135745  1.000020161
## 
## [[5]]
## [1] -4.307792e-05  1.000075e+00
## 
## [[6]]
## [1] -0.0001291289  0.9998602945
## 
## [[7]]
## [1] -6.030875e-05  1.000221e+00
## 
## [[8]]
## [1] -0.0000361746  1.0000757751
## 
## [[9]]
## [1] -0.0001635208  1.0000274952
## 
## [[10]]
## [1] -0.0001460433  0.9999632778</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">valsImplicit &lt;-<span class="st"> </span><span class="kw">do.call</span>(cbind, <span class="kw">as.list</span>(out))
<span class="kw">identical</span>(valsImplicit, valsExplicit)</code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="using-foreach" class="slide section level1">
<h1>8.3. Using foreach</h1>
<p>See the example code in <code>help(doFuture)</code> for template code on how to use the <code>%doRNG%</code> operator with foreach to ensure correct RNG with foreach.</p>
</div>
<div id="submitting-slurm-jobs-from-future-using-batchtools" class="slide section level1">
<h1>9. Submitting SLURM jobs from future using batchtools</h1>
<p>We can use the <code>future.batchtools</code> package to submit jobs to a cluster scheduler from within R.</p>
<p>One downside is that this submits one job per worker. On clusters (such as Savio) that schedule an entire node at once, that won't work.</p>
<p>On the SCF it is fine (so long as you don't have, say, tens of thousands of jobs). Here's an example. Note that the <code>resources</code> argument tells what the SLURM arguments should be for <em>each</em> worker.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(future.apply)
<span class="kw">library</span>(future.batchtools)
## Starts five workers as separate jobs.
<span class="kw">plan</span>(batchtools_slurm, <span class="dt">workers =</span> <span class="dv">5</span>,
                       <span class="dt">resources =</span> <span class="kw">list</span>(<span class="dt">nodes =</span> <span class="st">&quot;1&quot;</span>, <span class="dt">ntasks =</span> <span class="st">&quot;1&quot;</span>,
                       <span class="dt">cpus_per_task =</span> <span class="st">&quot;1&quot;</span>, <span class="dt">walltime =</span> <span class="st">&quot;00:05:00&quot;</span>),
                       <span class="dt">template =</span> <span class="st">&quot;batchtools.slurm.tmpl&quot;</span>)

output &lt;-<span class="st"> </span><span class="kw">future_sapply</span>(<span class="dv">1</span>:<span class="dv">100</span>, function(i) <span class="kw">mean</span>(<span class="kw">rnorm</span>(<span class="fl">1e7</span>)), <span class="dt">future.seed =</span> <span class="dv">1</span>)</code></pre></div>
</div>
<div id="submitting-slurm-jobs-that-are-allocated-per-node" class="slide section level1">
<h1>9.1. Submitting SLURM jobs that are allocated per node</h1>
<p>You can use nested futures to deal with the one job per worker issue. Here the outer future is just a wrapper to allow the overall code to be run within a single SLURM job.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(future.apply)
<span class="kw">library</span>(future.batchtools)
numWorkers &lt;-<span class="st"> </span><span class="dv">5</span>
## five workers
<span class="kw">plan</span>(<span class="kw">list</span>(<span class="kw">tweak</span>(batchtools_slurm, <span class="dt">workers =</span> <span class="dv">1</span>,
                       <span class="dt">resources =</span> <span class="kw">list</span>(
                                 <span class="dt">nodes =</span> <span class="st">&quot;1&quot;</span>,
                                 <span class="dt">ntasks =</span> <span class="kw">as.character</span>(numWorkers),
                                 <span class="dt">cpus_per_task =</span> <span class="st">&quot;1&quot;</span>,
                                 <span class="dt">partition =</span> <span class="st">&quot;high&quot;</span>,
                                 <span class="dt">walltime =</span> <span class="st">&quot;00:05:00&quot;</span>),
                       <span class="dt">template =</span> <span class="st">&quot;batchtools.slurm.tmpl&quot;</span>),
          <span class="kw">tweak</span>(multiprocess, <span class="dt">workers =</span> numWorkers)))

myfuture &lt;-<span class="st"> </span><span class="kw">future</span>({ <span class="kw">future_sapply</span>(<span class="dv">1</span>:<span class="dv">100</span>, function(i) <span class="kw">mean</span>(<span class="kw">rnorm</span>(<span class="fl">1e7</span>)),
                                   <span class="dt">future.seed =</span> <span class="dv">1</span>) })
out &lt;-<span class="st"> </span><span class="kw">value</span>(myfuture)</code></pre></div>
<p>While this is feasible, I prefer to set up my cluster jobs outside of R and have the R code not have to know anything about how the scheduler works or what scheduler is available on a given cluster.</p>
</div>
<div id="futurizing-your-code" class="slide section level1">
<h1>10. Futurizing your code</h1>
<p>Of course even with the future package one would generally need to write the code in anticipation of what might be parallelized.</p>
<p>However, in the case of lapply and sapply, you could even do this to &quot;futurize&quot; someone else's code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lapply &lt;-<span class="st"> </span>future_lapply
sapply &lt;-<span class="st"> </span>future_sapply</code></pre></div>
<p>and then just set a plan and run, since the arguments to <code>future_lapply</code> are the same as <code>lapply</code>.</p>
<p>Note it's not possible to do this with <code>parLapply</code> as it requires passing a cluster object as an argument, but something like this would be possible with <code>mclapply</code>.</p>
</div>
</body>
</html>
